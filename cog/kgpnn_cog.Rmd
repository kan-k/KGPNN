---
title: "cognitive_modelling"
author: "Kan Keeratimahat"
date: "10/07/2024"
output: html_document
---

#9 July

I plan to include a range of cognitive performance from subjects rather than age, see how I used depind below and do something similar

##Looking at the new file `ukb_latest-Cog.tsv`
20016 FLuid Intelligence (normally dist, 0-13)
6348  Duration to complete numeric path (trail #1) => left skewed 92 -2557
6350 	Duration to complete alphanumeric path (trail #2) => left skewed 160-2988
6373  Number of puzzles correctly solved => right skewed 0 - 15

```{r}
cog.dat<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Cog.tsv', sep = '\t', header = TRUE) #46535 53
summary(cog.dat)
#many missing values
```
The idea is to grab the mean of xxx.1.0 and xxx.2.0 and xxx.3.0 to average it.
```{r}
#fluid intelligence
cog.dat$fi <- apply(cbind(cog.dat$X20016.0.0,cog.dat$X20016.1.0,cog.dat$X20016.2.0,cog.dat$X20016.3.0),1,FUN = mean,na.rm=TRUE) 
#numeric path
cog.dat$num <- apply(cbind(cog.dat$X6348.2.0,cog.dat$X6348.3.0),1,FUN = mean,na.rm=TRUE) 
#alphanumeric
cog.dat$alphnum <- apply(cbind(cog.dat$X6350.2.0,cog.dat$X6350.3.0),1,FUN = mean,na.rm=TRUE) 
#puzzles
cog.dat$puzzle <-  apply(cbind(cog.dat$X6373.2.0,cog.dat$X6373.3.0),1,FUN = mean,na.rm=TRUE) 
```

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/age_sex_strat_depind.feather'))

#something along bottom line
for(i in 1:length(age_tab$id)){
  ind <- cog.dat$eid_8107==sub(".", "",age_tab$id[i])
  age_tab$fi[i] <- cog.dat$fi[ind]
  age_tab$num[i] <- cog.dat$num[ind]
  age_tab$alphnum[i] <- cog.dat$alphnum[ind]
  age_tab$puzzle[i] <- cog.dat$puzzle[ind]
  
} 
```
```{r}
summary(age_tab$fi)
summary(age_tab$num)
summary(age_tab$alphnum)
summary(age_tab$puzzle)
```

There are so many missing values. I think I will have to create another dataset for this.

Original data
```{r}
part_list<-read.table('/well/nichols/users/qcv214/Placement_2/participant_list.txt', header = FALSE, sep = "", dec = ".") #4529 participants
part_list$exist_vbm <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_list[,1],'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz'))
#These two are equal
part_use<-part_list[part_list$exist_vbm==1,] #4262 participants left
```

Extended data
```{r}
part_list2 <- read.csv('/well/nichols/users/qcv214/bnn2/add_1_part_id_use_final.txt')$V1 #4258
part_list2.exist_vbm <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',part_list2,'/T1/T1_vbm/T1_GM_to_template_GM_mod.nii.gz'))
part_use2 <-part_list2[part_list2.exist_vbm] #length = 4257
```

```{r}
part.combined <- c(part_use$V1,part_use2)
```

```{r}
agetab<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Age.tsv', sep = '\t', header = TRUE)
age_tab<-as.data.frame(matrix(,nrow = length(part.combined),ncol = 3)) #id, age, number of masked voxels
colnames(age_tab)[1:3]<-c('id','age','sex')
age_tab$id<-part.combined
# for(i in 1:length(part.combined)){
#   age_tab$age[i]<-agetab$X21003.2.0[agetab$eid_8107==sub(".", "",age_tab$id[i])]
#   age_tab$sex[i]<-agetab$X31.0.0[agetab$eid_8107==sub(".", "",age_tab$id[i])]
# }
```

###Deprivation index
```{r}
confs.dat<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Confs.tsv', sep = '\t', header = TRUE) #46535 53
confs.dat$combDepInd <- apply(cbind(confs.dat$X26410.0.0,confs.dat$X26427.0.0,confs.dat$X26426.0.0),1,FUN = mean,na.rm=TRUE) 
```


For the above, I should combine it with cognitive
###Cognitive
```{r}
cog.dat<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Cog.tsv', sep = '\t', header = TRUE) #46535 53
summary(cog.dat)
#many missing values
```
The idea is to grab the mean of xxx.1.0 and xxx.2.0 and xxx.3.0 to average it.
```{r}
#fluid intelligence
cog.dat$fi <- apply(cbind(cog.dat$X20016.0.0,cog.dat$X20016.1.0,cog.dat$X20016.2.0,cog.dat$X20016.3.0),1,FUN = mean,na.rm=TRUE) 
#numeric path
cog.dat$num <- apply(cbind(cog.dat$X6348.2.0,cog.dat$X6348.3.0),1,FUN = mean,na.rm=TRUE) 
#alphanumeric
cog.dat$alphnum <- apply(cbind(cog.dat$X6350.2.0,cog.dat$X6350.3.0),1,FUN = mean,na.rm=TRUE) 
#puzzles
cog.dat$puzzle <-  apply(cbind(cog.dat$X6373.2.0,cog.dat$X6373.3.0),1,FUN = mean,na.rm=TRUE) 
```

```{r}

#something along bottom line
for(i in 1:length(age_tab$id)){
  age_tab$age[i]<-agetab$X21003.2.0[agetab$eid_8107==sub(".", "",age_tab$id[i])]
  age_tab$sex[i]<-agetab$X31.0.0[agetab$eid_8107==sub(".", "",age_tab$id[i])]
  
  ind.dep <- confs.dat$eid_8107==sub(".", "",age_tab$id[i])
  age_tab$DepInd[i] <- confs.dat$combDepInd[ind.dep]
  
  ind <- cog.dat$eid_8107==sub(".", "",age_tab$id[i])
  age_tab$fi[i] <- cog.dat$fi[ind]
  age_tab$num[i] <- cog.dat$num[ind]
  age_tab$alphnum[i] <- cog.dat$alphnum[ind]
  age_tab$puzzle[i] <- cog.dat$puzzle[ind]
  
} 
# summary(age_tab$fi)
# summary(age_tab$num)
# summary(age_tab$alphnum)
# summary(age_tab$puzzle)
# dim is 8519 x 8
```

remove rows wityh any NaN
```{r}
age_tab2 <- na.omit(age_tab)
dim(age_tab2)
```

View the data distribution

```{r}
age_tab2 <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat.feather'))


for(i in 2:ncol(age_tab2)){
  
  if(i %in% c(5,6,8)){
    titl <- paste0("histogram of : log-",colnames(age_tab2)[i])
    hist(log(age_tab2[,i]), main = titl, xlab = paste0("log-",colnames(age_tab2)[i]))
  }
    # } else{
    titl <- paste0("histogram of : ",colnames(age_tab2)[i])
    hist((age_tab2[,i]), main = titl, xlab = colnames(age_tab2)[i])
  # }
  
}
```
I think there is nothing to stratify (apart from sex, which can perhaps be stratify during selection) after doing log transformation on tasks
Stratify by sex
```{r}
library(dplyr)
# Check the counts of each sex category
sex_counts <- table(age_tab2$sex)
# Determine the minimum count
min_count <- min(sex_counts)
# Stratify the data
balanced_age_tab <- age_tab2 %>%
  group_by(sex) %>%
  sample_n(size = min_count)
#left with 5204 subjects
```

Sort age by sex
```{r}
# Sort the entire dataframe by age and group by sex
balanced_age_tab <- balanced_age_tab %>%
  split(.$sex) %>%
  lapply(function(x) x[order(x$age),]) %>%
  bind_rows()
```


Save the data
##Data
```{r}
# write_feather(balanced_age_tab, '/well/nichols/users/qcv214/KGPNN/cog/agesex_strat.feather')
```

```{r}
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat.feather'))
```

Note that 5204/2.6 gives 2001
```{r}
train.ind <- round(seq(1,5204,2.6)) #gives 2002
train.ind <- train.ind[-2001:-2002]

test.ind <- round(seq(2,5204,2.6)) #gives 2001
test.ind <- test.ind[-2001]

#test overlap
sum(train.ind %in% test.ind) # = 0, so no overlap


```

Look at the dist
Train
```{r}
for(i in 2:ncol(age_tab2)){
  
  if(i %in% c(5,6,8)){
    titl <- paste0("histogram of : log-",colnames(age_tab2)[i])
    hist(log(age_tab2[train.ind,i]), main = titl)
  } else{
    titl <- paste0("histogram of : ",colnames(age_tab2)[i])
    hist((age_tab2[train.ind,i]), main = titl)
  }
  
}
```

Test
```{r}
for(i in 2:ncol(age_tab2)){
  
  if(i %in% c(5,6,8)){
    titl <- paste0("histogram of : log-",colnames(age_tab2)[i])
    hist(log(age_tab2[test.ind,i]), main = titl)
  } else{
    titl <- paste0("histogram of : ",colnames(age_tab2)[i])
    hist((age_tab2[test.ind,i]), main = titl)
  }
  
}
```

```{r}
write.csv(train.ind, file = '/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv', row.names = FALSE)
write.csv(test.ind, file = '/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv', row.names = FALSE)
```




###Some note on stratifying
####Stratify the data by using downsampling on sex
```{r}
# Check the counts of each sex category
sex_counts <- table(age_tab$sex)
# Determine the minimum count
min_count <- min(sex_counts)
# Stratify the data
balanced_age_tab <- age_tab %>%
  group_by(sex) %>%
  sample_n(size = min_count)
```
This `balanced_age_tab` is stratified correctly, and ordered by sex. amount is 1971 in each.

Sort age by sex
```{r}
# Sort the entire dataframe by age and group by sex
balanced_age_tab <- balanced_age_tab %>%
  split(.$sex) %>%
  lapply(function(x) x[order(x$age),]) %>%
  bind_rows()
```

```{r}
#write_feather(balanced_age_tab, '/well/nichols/users/qcv214/KGPNN/age_sex_strat.feather')
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/age_sex_strat.feather'))
```




Running 
[canceled] july10_num_sm_gpnn_init 678437 => modelling log(num) with 10 subclasses.  ===> it takes too long per epoch, i will change it to 250 search
//re_cog_july10_ridge (and lasso) 680547 => ridge and lasso with non-imaging... accidentally saved in the old folder
//july10_num_sm_gpols_init 681168 


##Load up ridge and lasso results
```{r}
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/pile/re_cog_july10_lasso_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/pile/re_cog_july10_ridge_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

```
Lasso is better on held-out than ridge. Interesting

#11 July

##Plot param search of gpolsoptions(bitmapType='cairo')
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july10_num_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,.5) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,.5) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
There doesnt'seem to be any learning overtime. Let's try bbs.

running
[Wrong] july10_num_sm_gpols_init_bbs 809259  ===> it's WRONG I was evaluating age but not lognum. I'll delete it
//july10_num_sm_gpols_init_bbs 1034923
//july10_num_sm_gpnn_init 809303 => modelling log(num) with 10 subclasses.  250 search



#12 July
## gpols bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july10_num_sm_gpols_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(log(age_tab$num)), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.4) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(log(age_tab$num)), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.4) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
It shouldn't be shotting up like this, this is unacceptable results.  ****



##Gpnn
##Plot param search of gpolsoptions(bitmapType='cairo')
```{r}
library(ggplot2)
library(tidyr)

num.it <- 250*4
runs <- c(3,5:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july10_num_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.5) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.5) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
This still doesn't beat linear model. let's try bbs and see

Running
[1,5,6,7,8,10] //july10_num_sm_gpnn_init_bbs. 1038515
```{r}
library(ggplot2)
library(tidyr)

num.it <- 250*4
runs <- c(1,5,6,7,8,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july10_num_sm_gpnn_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(log(age_tab$num)), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.4) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(log(age_tab$num)), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.2,.4) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
why is GPNN worse than GPOLS

Let's look at sd of our problem

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat.feather'))
#age_tab <- age_tab[order(age_tab$id),].     #DOES THIS MESS UP ORDER
lognum <- log(age_tab$num)
print(sd(lognum))
```

What if we try modelling the raw values instead of log num? perhaps non-gaussian assumption can make gpnn standout?

Running
july14_rawnum_sm_gpnn_init 1152691
july14_rawnum_sm_gpols_init 1152693
re_rcog_july14_ridge (and lasso) 1166295

#15 July

Doing `raw num` instead of `log num`

##Load up ridge and lasso results
```{r}
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rcog_july14_lasso_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rcog_july14_ridge_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

```
Lasso held out is better than held in??

##gpols
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july14_rawnum_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(60,100) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(40,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

##gpnn
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(2,6,9)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july14_rawnum_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(70,100) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

All these seem to be overfitting, I will reduce number of latent classes to 3

[2,4,5,7,8]july15_rawnum_sm_gpnn_init 1174402
//july15_rawnum_sm_gpols_init 1173756 

We can try to model alpha-num 
july15_rawalp_sm_gpols_init 1177261
july15_rawalp_sm_gpnn_init 1177288

#16 July
## july15_rawnum_sm_gpols_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(1:5,7:8)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july15_rawnum_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(70,100) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
reducing number of classes doesn't help gpnn.

##july15_rawnum_sm_gpols_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:9
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july15_rawnum_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(70,100) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$num), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
reducing number of classes doesn't help gpnn.

##Raw Alpha - Numeric
###july15_rawalp_sm_gpols_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july15_rawalp_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$alphnum), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(70,400) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$alphnum), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,400) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(2,4)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july15_rawalp_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$alphnum), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(70,400) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$alphnum), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,400) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

Does

#Need to grab another variable
See `https://www.ukbiobank.ac.uk/enable-your-research/approved-research/identifying-genetic-factors-for-brain-ageing`
Pairs matching task => bimodal.
I copied `/well/nichols/scripts/Extract_Cog2.sh` and run it using `bash ` and save everything to the folder `/well/nichols/users/qcv214/KGPNN/cog/Cog_dat`

##Loading
```{r}
cog.dat<-read.table(file = '/well/nichols/users/qcv214/KGPNN/cog/Cog_dat/ukb_latest-Cog2.tsv', sep = '\t', header = TRUE) #46535 53
# summary(cog.dat)
#many missing values
colnames(cog.dat)
```

Comparing
```{r}
cog.dat2<-read.table(file = '/well/nichols/projects/UKB/SMS/ukb_latest-Cog.tsv', sep = '\t', header = TRUE) #46535 53
# summary(cog.dat)
#many missing values
```

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat.feather'))
```


Pair Matching task
***Note that I am using the median across all, probably in real use, we wanna use latest info,
```{r}
# cog.dat$pm <- apply(cbind(cog.dat$X400.0.1, cog.dat$X400.0.2, cog.dat$X400.1.1, cog.dat$X400.1.2, cog.dat$X400.2.1, cog.dat$X400.2.2, cog.dat$X400.2.3, cog.dat$X400.3.1, cog.dat$X400.3.2, cog.dat$X400.3.3 ),1,FUN = mean,na.rm=TRUE) 
cog.dat$pm <- apply(cbind(cog.dat$X400.2.1, cog.dat$X400.2.2, cog.dat$X400.2.3 ),1,FUN = mean,na.rm=TRUE) 
```

There is 
```{r}

#something along bottom line
for(i in 1:length(age_tab$id)){
  ind <- cog.dat$eid_8107==sub(".", "",age_tab$id[i])
  age_tab$pm[i] <- cog.dat$pm[ind]
  
} 
summary(age_tab$pm)
# dim is 8519 x 8
```
there is 94 (prev 1 on full median) missing value, I will imput it by median
```{r}
age_tab$pm[which(is.na(age_tab$pm))] <- median(age_tab$pm, na.rm = TRUE)
```

```{r}
hist(age_tab$pm)
plot(density((age_tab$pm[age_tab$pm < 700])))
```
Somehow it's unimodal... we want bimodal
```{r}
# write_feather(age_tab, '/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather')
```

july16_rawpm_sm_gpols_init 1306485
july16_rawpm_sm_gpnn_init 1306504
re_rpm_july16_ridge (and lasso) 1386759
#17 July
##Pair matching
###Load up ridge and lasso results
```{r}
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_july16_lasso_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){xw
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_july16_ridge_noscale_",i,".csv"))))
}
# (hs.train <- quantile(hs.int[,1],c(0.25,0.5,0.75)))
# (hs.test <-quantile(hs.int[,3],c(0.25,0.5,0.75)))

(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

```

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```

###july16_rawpm_sm_gpols_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july16_rawpm_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(100,120) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(50,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
it's worse than the noise on held-out, 

###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(1:7)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july16_rawpm_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(100,120) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(50,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

I just realised that the training and test sd are entirely different.

I want to tr

Ridge and Lasso still beat our algorithm

##to do
I wanna run the imaging-only version for all these

//re_rpm_imaging_july16_ridge (and lasso)  1441828

//[1:4,6:9] july16_rawpm_gpnn_init 1608446

###Load up ridge and lasso results with imaging only
```{r}
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_july16_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_july16_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))
print("===== below with with imaging only =====")
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_imaging_july16_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_rpm_imaging_july16_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

```
Imaging only is better for ridge but worse for lasso. Ridge now beats lasso.

###GPNN Imaging only
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(1:4,6:9)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july16_rawpm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(100,120) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(50,100) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
Still worse than ridge and lasso, converge at response sd
Basically SAME as k-GPNN

#18 july

##Inverse nornmal transformation
```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
age_tab$pm_tf <- qnorm((rank(age_tab$pm,na.last="keep")-0.5)/sum(!is.na(age_tab$pm)))
# write_feather(age_tab, '/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather')
```


###UPDATE 8 AUG 24
I am applying inverse transform on held-in, keep the pre-learn transformation and apply it onto held-out.

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
```{r}
# Load necessary packages
if (!require(scales)) install.packages("scales")
if (!require(caret)) install.packages("caret")
library(scales)
library(caret)
library(ggplot2)

# Assuming age_tab and train.test.ind are already defined with the response variable called 'pm'

# Function to perform Gaussian Inverse Transformation on training data
gaussian_inverse_transform_train <- function(response_variable) {
  # Rank transformation
  ranks <- rank(response_variable)
  
  # Normal quantile transformation
  normal_quantiles <- qnorm((ranks - 0.5) / length(ranks))
  
  return(list(
    transformed = normal_quantiles,
    ecdf_fn = ecdf(response_variable)  # Empirical CDF function for the training data
  ))
}

# Apply pre-learned transformation to new data
apply_prelearned_transformation <- function(test_response_variable, ecdf_fn) {
  # Apply the ECDF to the test data
  ecdf_values <- ecdf_fn(test_response_variable)
  
  # Replace exact 0 and 1 values
  ecdf_values[ecdf_values == 0] <- .Machine$double.eps
  ecdf_values[ecdf_values == 1] <- 1 - .Machine$double.eps
  
  # Normal quantile transformation using the ECDF values
  normal_quantiles <- qnorm(ecdf_values)
  
  return(normal_quantiles)
}

# Learn the transformation on the training set
train_indices <- train.test.ind$train
test_indices <- train.test.ind$test

train_lognum <- age_tab$pm[train_indices]

# Apply the Gaussian Inverse Transformation on the training set
train_transformation <- gaussian_inverse_transform_train(train_lognum)

# Add the transformed training values back to the dataframe
age_tab$pm_tf <- NA
age_tab$pm_tf[train_indices] <- train_transformation$transformed

# Apply the learned transformation on the test set
test_lognum <- age_tab$pm[test_indices]
test_transformed <- apply_prelearned_transformation(test_lognum, train_transformation$ecdf_fn)

# Add the transformed test values back to the dataframe
age_tab$pm_tf[test_indices] <- test_transformed

# write_feather(age_tab, '/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather')
```


```{r}
hist(age_tab$pm_tf)
plot(density(age_tab$pm_tf))
```
This looks almost perfectly normal


Look at densities of train and test
```{r}
plot(density(age_tab$pm[train.test.ind$train]), main = "Density Plot of Pair Matching",
     xlab = "pm", ylab = "Density", col = "blue", lwd = 2)
lines(density(age_tab$pm[train.test.ind$test]), col = "red", lwd = 2)
legend('topright',legend = c('Training','Test'),lty=c(1),col=c('blue','red'))

```
```{r}
plot(density(age_tab$pm_tf[train.test.ind$train]), main = "Density Plot of Normal-inversed transformed of Pair Matching",
     xlab = "transformed pair matching", ylab = "Density", col = "blue", lwd = 2)
lines(density(age_tab$pm_tf[train.test.ind$test]), col = "red", lwd = 2)
legend('topright',legend = c('Training','Test'),lty=c(1),col=c('blue','red'))
```

##let's model this new pm_tf

//re_pm_july16_ridge (and lasso) 1492677
//[2:5,7,8,10]july18_pm_sm_gpnn_init 1492725
//july18_pm_sm_gpols_init 1492746
//[1:3,5,6,9,10]july18_pm_gpnn_init 1609389

#19 July 
```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_july16_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:1
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_july16_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july22_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```

###july16_rawpm_sm_gpols_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- 1:10
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july18_pm_sm_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.9,1.5) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.7,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

###july18_rawpm_sm_gpnn_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(2:5,7,8,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july18_pm_sm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +

    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.97,1.15) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

###july18_rawpm_gpnn_init
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(1:3,5,6,9,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july18_pm_gpnn_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.97,1.15) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
Okay this isn't bad actually.
I think k-GPNN is similar to gpnn.

Let's run BBS. (may try bbs ridge?)
//[4,5,7] july18_pm_sm_gpnn_init_bbs. 1787834

#22 July
###july18_rawpm_sm_gpnn_init bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(4,5,7)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july18_pm_sm_gpnn_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.9,1.5) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

It's weird. It only learns the noise, even tho minimum should really be same as lasso and ridge.
I think Train vs Test plots show that as soon as it overfits, the algorithm just captures noise.

I will reduce number of regions

##To do
I think I should tune the number hidden neurons down for this version.

GPR
//re_july22_pm_gpr  1895497 


#24 july
// [c(1,3:8,10)]july24_pm_sm_gpnn_12init  2214055 => reduce from 53 regions to 12 ===> 10 hours (reduced from 30)

```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1,3:8,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july24_pm_sm_gpnn_12init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

Run bbs 
//july24_pm_sm_gpnn_12init_bbs  2287225


##bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(2:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july24_pm_sm_gpnn_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```


#26 july

##To do

1. Run gpols but with 12 number of regions (and 20 degs)
//july26_pm_sm_gpols_12init  2496385
2.Try bimodal


##gpols param search

```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_sm_gpols_12init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```



##biomodal data

```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```

#check for existing Default Mode Network
```{r}
age_tab$dmn <- file.exists(paste0('/well/win-biobank/projects/imaging/data/data3/subjectsAll/',age_tab$id,'/fMRI/rfMRI_25.dr/dr_stage2.nii.gz')) #4545 subjects, 87%
age_tab_dmn <- age_tab[age_tab$dmn == TRUE, ]
# write_feather(age_tab_dmn, '/well/nichols/users/qcv214/KGPNN/cog/age_dmn_sex_strat.feather')
```

```{r}
train.ind <- round(seq(1,4545,2.27)) #gives 2002
train.ind <- train.ind[-2001:-2002]

# test.ind <- round(seq(2,4545,2.26)) #gives 2002
# test.ind <- test.ind[-2001:-2002]
test.ind <- train.ind+1 
#test overlap
sum(train.ind %in% test.ind) # = 897, so many overlaps
```
###12 August, correct inverse transform

```{r}
# Load necessary packages
if (!require(scales)) install.packages("scales")
if (!require(caret)) install.packages("caret")
library(scales)
library(caret)
library(ggplot2)

# Assuming age_tab and train.test.ind are already defined with the response variable called 'pm'

# Function to perform Gaussian Inverse Transformation on training data
gaussian_inverse_transform_train <- function(response_variable) {
  # Rank transformation
  ranks <- rank(response_variable)
  
  # Normal quantile transformation
  normal_quantiles <- qnorm((ranks - 0.5) / length(ranks))
  
  return(list(
    transformed = normal_quantiles,
    ecdf_fn = ecdf(response_variable)  # Empirical CDF function for the training data
  ))
}

# Apply pre-learned transformation to new data
apply_prelearned_transformation <- function(test_response_variable, ecdf_fn) {
  # Apply the ECDF to the test data
  ecdf_values <- ecdf_fn(test_response_variable)
  
  # Replace exact 0 and 1 values
  ecdf_values[ecdf_values == 0] <- .Machine$double.eps
  ecdf_values[ecdf_values == 1] <- 1 - .Machine$double.eps
  
  # Normal quantile transformation using the ECDF values
  normal_quantiles <- qnorm(ecdf_values)
  
  return(normal_quantiles)
}

# Learn the transformation on the training set
train_indices <- train.ind
test_indices <- test.ind

train_lognum <- age_tab_dmn$pm[train_indices]

# Apply the Gaussian Inverse Transformation on the training set
train_transformation <- gaussian_inverse_transform_train(train_lognum)

# Add the transformed training values back to the dataframe
age_tab_dmn$pm_tf <- NA
age_tab_dmn$pm_tf[train_indices] <- train_transformation$transformed

# Apply the learned transformation on the test set
test_lognum <- age_tab_dmn$pm[test_indices]
test_transformed <- apply_prelearned_transformation(test_lognum, train_transformation$ecdf_fn)

# Add the transformed test values back to the dataframe
age_tab_dmn$pm_tf[test_indices] <- test_transformed
# write_feather(age_tab_dmn, '/well/nichols/users/qcv214/KGPNN/cog/age_dmn_sex_strat.feather')
```

```{r}
plot(density(age_tab_dmn$pm_tf[train.ind]), main = "Density Plot of Normal-inversed transformed of Pair Matching",
     xlab = "transformed pair matching", ylab = "Density", col = "blue", lwd = 2)
lines(density(age_tab_dmn$pm_tf[test.ind]), col = "red", lwd = 2)
legend('topright',legend = c('Training','Test'),lty=c(1),col=c('blue','red'))
```


```{r}
# write.csv(train.ind, file = '/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_train_index.csv', row.names = FALSE)
# write.csv(test.ind, file = '/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_test_index.csv', row.names = FALSE)
```

//[2,4,8,9] july26_pm_bi_gpnn_init 2586241  (16 slots)
//july26_pm_sm_gpols_12init_bbs  2508689


##GPOLS bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_sm_gpols_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.6,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

In some iteration, it DOES beat lasso and ridge. Interesting.
Look at run 3,5,6,10

Maybe try GPNN GP GP???

//[1,4,6:10] july26_pm_sm_gpgp_12init 2600564 ==> takes 2 hours (compared to 1 hours of gpols)

##gpgp param search
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1,4,6:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_sm_gpgp_12init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.6,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

// [1,2,3,5,6,10]july26_pm_sm_gpgp_12init_bbs. 2607330


##gpgp bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1,2,3,5,6,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_sm_gpgp_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.6,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
gpgp isn't good.




//[] july26_pm_bi_gpnn_init_bbs 2684755


#28 july

##gpgp bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(2,8,9,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_bi_gpnn_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

This is interesting, it can achieve rmse of 0 while maintaining decent test rmse.

#4 aug

##To do

1. Run GPNN without latent class, but with 12 number of regions
aug4_pm_gpnn_12init 3448243

2. observe the stability and interpretability of latent classes in bbs

##Observe class assignment 

###
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- 1:10

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_july26_pm_sm_gpols_12init_bbs_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd','pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$age)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd','pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError,pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

3,5,6,10 from good held-out RMSE


#6 Aug

##GPNN without latent class
###param search
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:8,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug4_pm_gpnn_12init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

//aug4_pm_gpnn_12init_bbs  3705710

#7 Aug
##GPNN Plot bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(2,3,6,9)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug4_pm_gpnn_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.3) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
GPNN without latent classes overfits too easily. This is good findings, meaning the k-GPNN is better

Is it OLS-GP that I need to compare it to tho??

//aug7_pm_gpols_12init. 3869534

##GPOLS without latent class
###param search
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug7_pm_gpols_12init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```


//aug7_pm_gpols_12init_bbs  3876485
##GPOLS bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug7_pm_gpols_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
This is saying that WITHOUT latent class, gpols version is worse than k-gpols!!!

Note that the current test s.d. line isn't correct, I am to re-run the analysis on the new pm transformation.


#8 Aug

IMPORTANT:
I have overwrote the save of `agesex_strat2.feather`. I am applying inverse transform on held-in, keep the pre-learn transformation and apply it onto held-out.


I will re-run everything.

[1:10]//aug9_pm_gpols_12init  4090379
//re_aug9_pm_gpr_noscale_  4090389
//re_pm_aug9_ridge_noscale_ 4090399
[1:5,7,8,10]// aug9_pm_sm_gpgp_12init  4090427
[2,3,8:10]//aug9_pm_sm_gpnn_12init  4090462
[1:10]//aug9_pm_sm_gpols_12init  4090443
[1:4,8,9]//aug9_pm_bi_gpnn_init   4090467
aug9_pm_bi_gpols_init 4379501 ==> new

###bbs

[1:10] aug9_pm_gpols_12init_bbs 4363146
[1:8,10] aug9_pm_sm_gpgp_12init_bbs  4363162
[1,2,4,6,10] aug9_pm_sm_gpnn_12init_bbs  4363177
[1:10]aug9_pm_sm_gpols_12init_bbs  4363170
[2,7,9] aug9_pm_bi_gpnn_init_bbs   4363181

#9 aug 
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```

##Now do all the plotting
[1:10] aug9_pm_gpols_12init_bbs 4363146
[1:8,10] aug9_pm_sm_gpgp_12init_bbs  4363162
[1,2,4,6,10] aug9_pm_sm_gpnn_12init_bbs  4363177
[1:10]aug9_pm_sm_gpols_12init_bbs  4363170
[2,7,9] aug9_pm_bi_gpnn_init_bbs   4363181
###vanilla gpols Plot bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.88,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
###gpols bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(2:3,5:6,9)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.88,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
###gpgp bbs

```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:8,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
###gpnn bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1,2,4,6,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

####Observe class assignment 


```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(2,3,5,9)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd','pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd','pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError,pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(2, 3, 5, 9)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```



Actually I need to re-save stuff onto dmn response variable too, so it's wrong right now.

[2,6,7,9]//aug9_pm_bi_gpols_init. 4404012 ==> reduce max mag of lr
[3,4,10] aug9_pm_bi_gpnn_init 4403909


Note that I need to run GPR, LASSO, Ridge for bimodal

//re_aug13_bimodal_ridge. 4654683
//re_aug13_bimodal_pm_gpr 4652713
```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/age_dmn_sex_strat.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_train_index.csv')$x
```

###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```

##bi gpols
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(2,6,7,9)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.97,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```


##bi gpols
```{r}
library(ggplot2)
library(tidyr)

num.it <-250*4
runs <- c(3,4,10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpnn_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.97,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
#18 Aug

Testing sgld. 
I am only doing sgld on gpols and present it first to see if we need it. GPNN and GPGP vcan be done later

//aug9_pm_sm_gpols_12init_sgld 6246995
//aug9_pm_gpols_12init_sgld 6347708

###gpols sgld
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_sgld_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.88,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```

We can just pick a sensible run to assess, e.g. run 3

###gpols bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <-500*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_sgld_loss__jobid_",i,".csv")))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
resdat <- as.data.frame(sqrt(res.mat[2,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(sqrt(res.mat[1,,]))
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +
  geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
  geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  ylim(0.88,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```


#19 August

SGLD
###getting sgld results
```{r}
for(i in c(9)){
  print(i)
  res <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/summary_re_aug9_pm_sm_gpols_sgld_",i,".csv" ))$x
  names(res) <-  c("inRMSE","outRMSE","inR2","outR2","in-Coverage","out-Coverage","in-PPIwidth","out-PPIwidth","inMAE","outMAE", "G1", "G2")
  print(res)
  print("---")
}
```
There are clear distinctions between good and bad runs
Let's look at 7,9,10

Held-in
```{r}
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(2,3,5,9)

for(r in success.run){
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_sgld_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in,n.test/4*1000)
  colnames(dat.in) <- c('id_ind','pred','class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <-  dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in,n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

# Merge with age and sex information
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd','pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd','pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError,pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c( 9)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_sgld_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
Calculating the 2.5 and 97.5 IQR
```{r}
for(C in c(1,2,3)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
}
```

###getting sgld result of No-class GP-OLS
```{r}
for(i in c(9)){
  print(i)
  res <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/summary_re_aug9_pm_gpols_sgld_",i,".csv" ))$x
  names(res) <-  c("inRMSE","outRMSE","inR2","outR2","in-Coverage","out-Coverage","in-PPIwidth","out-PPIwidth","inMAE","outMAE")
  print(res)
  print("---")
}
```


#22 Aug

//aug22_pm_sm_gpols_12init_K. 8191439 ==> K = 2,4,5
//aug22_pm_sm_gpols_12init_K_bbs  8315986  ==> K = 2,4,5

//aug22_pm_sm_gpols_12init_K. 8315998 ==> K = 8,12,15
//aug22_pm_sm_gpols_12init_K_bbs 8317449==> K = 8,12,15

###gpols bbs
#### varying K
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2, 4, 5)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

We can already see here that as K increases, training RMSE goes down whereas test RMSE go up.

Let's look at the class of K = 2
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

The split are pretty consistent. Not 100% overlap between runs but I think very close enough. All class dists are very similar,

I think the rule is to increase K until class dist get unstable.

#26 August
###gpols bbs
#### varying K = 8,12,15
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2, 4, 5,8,12,15)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
For 8,12,15, the results are basically indisnguishable, they are not bad tho but worse than K = 2
Let's look at the class of K = 8
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K15_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
So looking at K = 8 and K = 15, the max number of classes are 6. It no longer exhaust all number of classes.


Runnning
//aug22_pm_sm_gpols_12init_sgld 9686098

GLD
###getting sgld results
```{r}
for(i in c(1:10)){
  print(i)
  res <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/summary_re_aug22_pm_gpols_sgld_K2_",i,".csv" ))$x
  names(res) <-  c("inRMSE","outRMSE","inR2","outR2","in-Coverage","out-Coverage","in-PPIwidth","out-PPIwidth","inMAE","outMAE", "G1", "G2")
  print(res)
  print("---")
}
```

Run 4 has terrible held-out
All runs seem pretty consistent

Held-in

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
```{r}
for(C in c(1,2)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
}
```
Run 2 is a good example, maybe 10 as well
1 tries to split using depind and gender
run 4 is funniest on held0out but is most interpretable (same interpret as bbs) ==> it actually looks fine on held-out below.
held-out
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_outpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```



#9 Sep

##To do
- Observe different runs of sgld
- Look at mcmc convergence
  - coda library and use mcmc diag on prediction. (maybe held-in, rather than out?)

##Results of SGLD in many runs
All runs have similar coverage performance

For run 1:8,10 since 9 only has 1 class
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:8)

for (r in success.run) {
  print(paste0("======== done ======", r))
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  for(C in c(1,2)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
  }
  
}
```
run 9 
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(9)

for (r in success.run) {
  print(paste0("======== done ======", r))
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  for(C in c(1)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
  }
  
}
```

###Convergence

I will devise my own detection algorithm.
1. from mcmc_object. Observe sd(first 100) - sd(last 100) for each column.
2. Count number of cols which this value is positive



ChatGPT fix

```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:500  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(1, ind)
mcmc_object2 <- process_mcmc_object(3, ind)
mcmc_object3 <- process_mcmc_object(2, ind)
mcmc_object4 <- process_mcmc_object(5, ind)
# mcmc_object1 <- process_mcmc_object(1, ind)
# mcmc_object2 <- process_mcmc_object(2, ind)
# mcmc_object3 <- process_mcmc_object(3, ind)
# mcmc_object4 <- process_mcmc_object(5, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2, mcmc_object3, mcmc_object4)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2) |
                          check_zero_variance(mcmc_object3) |
                          check_zero_variance(mcmc_object4)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list

}
# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
100% converged, based on 500 training subjects
if not scaling, only 13.8% have converged on the "good" runs
but on bad, it's 4.8


It cannot run the test with >= 1000 subjects

```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:57  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_outpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(1, ind)
mcmc_object2 <- process_mcmc_object(3, ind)
mcmc_object3 <- process_mcmc_object(6, ind)
mcmc_object4 <- process_mcmc_object(10, ind)

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2, mcmc_object3, mcmc_object4)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2) |
                          check_zero_variance(mcmc_object3) |
                          check_zero_variance(mcmc_object4)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
For held-out it's also 100, tested on 57 subjects (note that heldout has 2k iterations instead of 500 like held-in)


Try matching 2,10 then 7,8 based on boxplots

```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(3, ind)
mcmc_object2 <- process_mcmc_object(10, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2) |

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
pair (2,10) with 400 subjects corr 32.5%
pair (7,8) with 400 subjects corr 16.5%
let's try a random pair (3,10) => 24.75 wtf


#9 sep
SGLD:
I accidentally used 0.99/it.num as lr, should have used 1e-6
Now running
//aug22_pm_sm_gpols_12init_sgld_lr 10716591 => changed lr, and let it run for 750 epochs while recording only last 500

Held-in

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, and DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(2, ind)
mcmc_object2 <- process_mcmc_object(10, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```

#10 sep

traceplot of a few subjects
```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Reduced number of subjects for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(2, ind)
mcmc_object2 <- process_mcmc_object(10, ind)

# Pick 10 subjects (columns) from the two chains
num_subjects <- 10
subject_indices <- sample(1:ncol(mcmc_object1), num_subjects)  # Randomly sample 10 subjects

# Create an mcmc.list object with the selected subjects
mcmc.chain <- mcmc.list(
  mcmc_object1[, subject_indices], 
  mcmc_object2[, subject_indices]
)

# Plot the traceplots for the selected subjects using the coda package
plot(mcmc.chain)
```

544, 92, 680, 526, 916, 947, 253, 248, 219, 474

##low lr sgld
###getting sgld results
```{r}
for(i in c(1:10)){
  print(i)
  res <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/summary_re_aug22_pm_gpols_sgld_lr_K2_",i,".csv" ))$x
  names(res) <-  c("inRMSE","outRMSE","inR2","outR2","in-Coverage","out-Coverage","in-PPIwidth","out-PPIwidth","inMAE","outMAE", "G1", "G2")
  print(res)
  print("---")
}
```
For run 1:8,10 since 9 only has 1 class
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:3,5,6,10)

for (r in success.run) {
  print(paste0("======== done ======", r))
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  for(C in c(1,2)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
  }
  
}
```

Held-out
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:3,5,6,10)

for (r in success.run) {
  print(paste0("======== done ======", r))
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_K2_outpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  for(C in c(1,2)){
  quants95 <- quantile(age.in[age.in$class==C,'pm_tf'], probs = c(0.025,0.975))
  print(quants95[2]-quants95[1])
  }
  
}
```

#sep 22
adjusting the MAP inside`pm_sm_gpols_12init_K_sgld` by deleting the prior term on hidden layer as we are using lm()
sep22_pm_sm_gpols_12init_sgld_lr  12714361

let's look at loss
##MAP of sgld
```{r}
library(ggplot2)
library(tidyr)

num.it <-750*4
runs <- c(1:10)
res.mat <- array(,dim=c(2,length(runs),num.it))
for(i in runs){
  res.mat[,which(i==runs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_",i,".csv"))))[,1:num.it]
 }

# Assuming resdat is your dataframe
# Add a row identifier
# resdat <- as.data.frame(sqrt(res.mat[2,,]))
resdat <- as.data.frame(res.mat[2,,])
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  # ylim(0.99,1.1) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "iterations", y = "Value", color = "Run number")

resdat <- as.data.frame(res.mat[1,,])
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))
# Reshape the data to long format
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
# Create the line plot
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  # ylim(0.88,1.1) +
  theme_minimal() +
  labs(title = "Train RMSE",x = "iterations", y = "Value", color = "Run number")
```
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
These plots show barely any deviation


Running
//sep22_pm_sm_gpols_12init_sgld_lr99  12778303
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "Iterations", y = "Value", color = "Run number")
```
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:3)
res.mat <- array(, dim = c(2, length(runs), num.it))+
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[, which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

```
##investigate similar runs

###low lr
For `sep22_pm_sm_gpols_12init_sgld_lr_K2`
Let's plot 5 and 7 to see if they overlap
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(5,7)
res.mat <- array(, dim = c(2, length(runs), num.it))
for(i in runs){
  res.mat[, which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat[2,,])  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "Iterations", y = "Value", color = "Run number")

# Plotting Train RMSE
resdat <- as.data.frame(res.mat[1,,])  # Use first row of res.mat (Train RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Train RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Train RMSE", x = "Iterations", y = "Value", color = "Run number")
```
```{r}
library(coda)
library(feather)
library(mcmc)
# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(5, ind)
mcmc_object2 <- process_mcmc_object(7, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
This is very bad convergence. Note that if I used the exact same runs on high lr, I got the exact same answer

traceplot of a few subjects
```{r}
library(coda)

# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Reduced number of subjects for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_sgld_lr_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(5, ind)
mcmc_object2 <- process_mcmc_object(7, ind)

# Pick 10 subjects (columns) from the two chains
num_subjects <- 10
subject_indices <- sample(1:ncol(mcmc_object1), num_subjects)  # Randomly sample 10 subjects

# Create an mcmc.list object with the selected subjects
mcmc.chain <- mcmc.list(
  mcmc_object1[, subject_indices], 
  mcmc_object2[, subject_indices]
)

# Plot the traceplots for the selected subjects using the coda package
plot(mcmc.chain)
```



###high lr
for `sep22_pm_sm_gpols_12init_sgld_lr99_K2` 
let's do 6,8
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(6,8)
res.mat <- array(, dim = c(2, length(runs), num.it))
for(i in runs){
  res.mat[, which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat[2,,])  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Test RMSE", x = "Iterations", y = "Value", color = "Run number")

# Plotting Train RMSE
resdat <- as.data.frame(res.mat[1,,])  # Use first row of res.mat (Train RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Train RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Train RMSE", x = "Iterations", y = "Value", color = "Run number")
```
```{r}
library(coda)
library(feather)
library(mcmc)
# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(6, ind)
mcmc_object2 <- process_mcmc_object(8, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
This is very bad convergence

traceplot of a few subjects
```{r}
library(coda)

# Pick 10 subjects (columns) from the two chains
num_subjects <- 10
subject_indices <- sample(1:ncol(mcmc_object1), num_subjects)  # Randomly sample 10 subjects

# Create an mcmc.list object with the selected subjects
mcmc.chain <- mcmc.list(
  mcmc_object1[, subject_indices], 
  mcmc_object2[, subject_indices]
)

# Plot the traceplots for the selected subjects using the coda package
plot(mcmc.chain)
```
Mixing seems alright here. I think higher lr might lead to good mixing??

##Saliency 
I did saliency plot for k-GP-OLS in 
//`sep24_pm_sm_gpols_12init_bbs`  13335761
I created `sep24_pm/` for saliency viz

Created
`pm_sm_gpols_12init_K_sgld_run.R` => running SGLD on a fixed initial point/run
//sep22_pm_sm_gpols_12init_sgld_lr99_run2 13515044

Finding similar runs with the same classes
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep24_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
It's all runs but run 9, so runs 1-8,10
Created a directory `sep24_similar`


#sep 26

##Convergence, same initial
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```

Now let's observe convergence 
```{r}
library(coda)
library(feather)
library(mcmc)
# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_run2_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(6, ind)
mcmc_object2 <- process_mcmc_object(8, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
Still very low convergence??
traceplot of a few subjects
```{r}
library(coda)

# Pick 10 subjects (columns) from the two chains
num_subjects <- 10
subject_indices <- sample(1:ncol(mcmc_object1), num_subjects)  # Randomly sample 10 subjects

# Create an mcmc.list object with the selected subjects
mcmc.chain <- mcmc.list(
  mcmc_object1[, subject_indices], 
  mcmc_object2[, subject_indices]
)

# Plot the traceplots for the selected subjects using the coda package
plot(mcmc.chain)
```

```{r}
library(coda)
library(feather)
library(mcmc)
# Subset the subjects to a smaller number to reduce dimensionality
ind <- 1:400  # Try reducing the number of subjects to 500 for diagnostic purposes

# Function to read, process, and create MCMC objects
process_mcmc_object <- function(job_id, ind) {
  theta.matrix <- as.matrix(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_run2_K2_inpred__jobid_', job_id, '.feather')))
  splt <- split(theta.matrix[2,], theta.matrix[1,])  # Group 2nd row based on 1st row into a list
  rs <- sapply(splt, c)  # Create time x subject matrix
  rs <- rs[, ind]  # Select subjects based on index
  
  # Scale the data to reduce numerical instability
  # rs <- scale(rs)
  
  mcmc_object <- as.mcmc(rs)
  return(mcmc_object)
}

# Process the MCMC objects for different runs
mcmc_object1 <- process_mcmc_object(6, ind)
mcmc_object2 <- process_mcmc_object(10, ind)

#1,2,3,5,6,10

# Combine the MCMC chains into a list
mcmc.chain <- mcmc.list(mcmc_object1, mcmc_object2)

# Check if there are any columns (subjects) with zero variance
check_zero_variance <- function(mcmc_object) {
  apply(mcmc_object, 2, function(x) var(x) == 0)
}

# Identify subjects with zero variance across chains
zero_variance_subjects <- check_zero_variance(mcmc_object1) |
                          check_zero_variance(mcmc_object2)

if (any(zero_variance_subjects)) {
  warning("Some subjects have zero variance in all chains. These subjects will be removed.")
  # Filter out subjects with zero variance
  mcmc.chain <- lapply(mcmc.chain, function(chain) {
    chain[, !zero_variance_subjects]
  })
  mcmc.chain <- mcmc.list(mcmc.chain)  # Rebuild the mcmc.list
}

# Perform Gelman-Rubin diagnostic
r.hat <- tryCatch({
  gelman.diag(mcmc.chain)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})

if (!is.null(r.hat)) {
  # Print the Gelman-Rubin diagnostic
  # print(r.hat)
  print(sum(r.hat$psrf[,1]<1.1)/nrow(r.hat$psrf)*100)

} else {
  message("Could not calculate Gelman-Rubin diagnostic.")
}
```
we can bump this number up on (pair with 10) to 44.5 on pair (6,10), lowest is 19.5 (7,10)

#29 Sep
Running Simulation. Let's try same thing as before.
Take a realisation run then fit the model. First, let's try a clear split between class

Find a sensible Run
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
They all look similar, let's arbitrarily pick Run 10
Load in that run
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', 10, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in <- dat.in[order(dat.in$id_ind), ]
  
#
  n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x)
  # Load and process data
  dat.out <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_outpred__jobid_', 10, '.feather'))))[, c(1:3)]
  dat.out <- tail(dat.out, n.test)
  colnames(dat.out) <- c('id_ind', 'pred', 'class')
  dat.out <- dat.out[order(dat.out$id_ind), ]
dat.full <- rbind(dat.in,dat.out)
dat.full <- dat.full[order(dat.full$id_ind), ]

age_tab$class <- NA
age_tab$pred <- NA
for(i in dat.full$id_ind){
  age_tab$class[i] <- dat.full$class[dat.full$id_ind==i]
  age_tab$pred[i] <- dat.full$pred[dat.full$id_ind==i]
}
#doing amplify
age_tab$pred_amp <- ifelse(age_tab$class == 1, age_tab$pred, 
                                   ifelse(age_tab$class == 2, age_tab$pred +2, NA))

write_feather(age_tab, '/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather')
```

Plotting
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

  class_order <- age_tab %>%
    group_by(class) %>%
    summarise(median_pred = median(pm_tf, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age_tab$class <- factor(age_tab$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age_tab %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class

  # Reshape the data for plotting
  age_tab_long <- pivot_longer(age_tab, cols = c(age, sex, DepInd, pm_tf,pred,pred_amp), names_to = "variable", values_to = "value")
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age_tab_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", 10),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
```

##To do
I think there 3 things to try
1. Model pred
2. Model pred_amp
3. Increase K and see if it can model the classes correctly.

`sim_pm_sm_gpols_K.R``sim_pm_sm_gpols_K_pred_amp.R`
Running
//sep29_pm_sm_gpols_pred_amp, K=2 and 4   14290220
//sep29_pm_sm_gpols_pred, K=2 and 4  14290222

//sep29_pm_sm_gpols_pred_amp_bbs 14292040
//sep29_pm_sm_gpols_pred_bbs 14292287

##Plotting pred_amp

```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```

###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_amp_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_amp_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_amp_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```


```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2, 4)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pred_amp[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.5, 1.3) +
    theme_minimal() +
    labs(title = paste("Test RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pred_amp[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0, 1.3) +
    theme_minimal() +
    labs(title = paste("Train RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Plot distribution
###K2
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files
library(mclust)
# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred_amp','class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred_amp)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_amp','class_true', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pred_amp,class_true), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```
it should split into 1097-903. Seems like there is actually an error, BUT all runs converge to the same splitting somehow.
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)  # For calculating ARI or other similarity metrics

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred_amp', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred_amp)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```


###K4
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)  # For calculating ARI or other similarity metrics

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_bbs_K4_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred_amp', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred_amp)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```
It make exhausts all 4 classes, which shouldn't be the case


##Plotting for pred
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_oct1_pred_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2, 4)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pred[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0 ,.5) +
    theme_minimal() +
    labs(title = paste("Test RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pred[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0, .5) +
    theme_minimal() +
    labs(title = paste("Train RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

###K2
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)  # For calculating ARI or other similarity metrics

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```

we don't see convergence here. Though the classes don't deviate too much from true values

###K4
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)  # For calculating ARI or other similarity metrics

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_bbs_K4_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```
The 4 classes here don't detect either

#30 Sep

I suppose we should also look at sgld? jian mentioned about observing posterior dist
//sep29_pm_sm_gpols_pred_sgld_lr99 14382222
//sep29_pm_sm_gpols_pred_amp_sgld_lr99 14382344


##SGLD
Let's look at K =2 for now

###pred amp
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:5,7:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Run 6 appear to fail.

Boxplot - without similarity. Similarity can be run using the external script
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_sgld_lr99_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
   # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred_amp', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred_amp)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```

###pred
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```

Boxplot
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_sgld_lr99_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
   # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```

All SGLD here are terrible, may need to tune down the learning rate from 99 to lower

Below is for lr = 1e-4
//sep29_pm_sm_gpols_pred_amp_sgld 14410364
//sep29_pm_sm_gpols_pred_sgld 14410373

###pred amp
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:5,7:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_sgld_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Run 6 appear to fail.

Boxplot - without similarity. Similarity can be run using the external script
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_amp_sgld_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
   # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred_amp', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred_amp)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```

###pred
```{r}
library(ggplot2)
library(tidyr)

num.it <- 750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_sgld_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```

Boxplot
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(mclust)

# Function to get the mode
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/sim_agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/sim_sep29_pm_sm_gpols_pred_sgld_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test / 4 * 1000)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  dat.in.grouped <- dat.in %>%
    group_by(id_ind) %>%
    summarize(pred_class = getmode(class))
  dat.in.grouped <- dat.in.grouped[order(dat.in.grouped$id_ind), ]
  dat.in <- tail(dat.in, n.test)
  dat.in <- dat.in[order(dat.in$id_ind), ]
  dat.in$class <- dat.in.grouped$pred_class

  # Merge with age and sex information
   # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pred', 'class')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pred)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pred_true', 'class_true', 'pred', 'SqError', 'class')
  
  # Calculate a similarity metric (e.g., Adjusted Rand Index - ARI)
  ari_value <- adjustedRandIndex(age.in$class_true, age.in$class)
  
  # Create a separate dataframe for the similarity metric (ARI) to be included as a single point
  similarity_data <- data.frame(
    class = factor(1),  # Just one "class" for the similarity metric
    value = ari_value,
    variable = "ARI"
  )
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, class_true, SqError, pred_true), names_to = "variable", values_to = "value")
  
  # Bind the ARI data to the rest of the data
  combined_data <- bind_rows(age.in_long, similarity_data)
  
  # Create the boxplot with class sizes in the legend and similarity metric
  p <- ggplot(combined_data, aes(x = class, y = value, fill = class)) +
    geom_boxplot(data = combined_data %>% filter(variable != "ARI"), show.legend = TRUE) +  # Show legend for original boxplots
    geom_point(data = combined_data %>% filter(variable == "ARI"), aes(x = class, y = value), color = "blue", size = 5, show.legend = FALSE) +  # No legend for ARI
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, and ARI for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
    theme(legend.position = "right")  # Keep the legend on the right
  
  print(p)
}
```

#1 Oct

Created ridge, pr, lasso for pred amp and pred
all under Oct 1
//`sim_pred_amp_ridgelassogpr.R` 14667241
//`sim_pred_ridgelassogpr.R` 14667242


#2 Oct
Look at Gelman-Rubin of MAP

##First diagnose the MAP jump
```{r}
library(ggplot2)
library(tidyr)

num.it <- 10#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
The jump doesn't happen instantly, but it happens in 5 steps and get stuck. This is already high lr.
Let's look at low lr 
```{r}
library(ggplot2)
library(tidyr)

num.it <- 20#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Effect of jump is less visible, but lr should be low enough not to make any significant changes.

diff init, high lr
```{r}
library(ggplot2)
library(tidyr)

num.it <- 50#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Look at rmse... but it's sgld
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 750 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.98 ,1.25) +
    theme_minimal() +
    labs(title = paste("Test RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0, .5) +
    theme_minimal() +
    labs(title = paste("Train RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

###Look at jumps of map vs rmse
diff init, high lr
```{r}
library(ggplot2)
library(tidyr)

num.it <- 50#750 * 4
runs <- c(7,9)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Look at rmse... but it's sgld
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 50 
runs <- c(7,9)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(1 ,1.15) +
    theme_minimal() +
    labs(title = paste("Test RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0, .5) +
    theme_minimal() +
    labs(title = paste("Train RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```


##Same initialisation
```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(1:10)
num.it <- 750*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```
All 10 chains have converged.

##Different init
```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(1:10)
num.it <- 750*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```
We observe terrible r^ here. 40

try with lower lr

```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(5,7)
num.it <- 750*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_sep22_pm_sm_gpols_12init_sgld_lr_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```
Even for the best pair with low lr (5,7), the g-r obtain 1.93 (no convergence)

From this, I believe that MCMC convergence will NOT work from our parameter search as the parameters may be in different neighbourhood initially.


#7 Oct

To observe convergence, let's try adding some noise to the initial params of run 2
//Oct7_pm_sm_gpols_12init_sgld_lr99_run2  15918763


#8 oct

##SGLD same start with jitters
```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(1:10)
num.it <- 750*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_Oct7_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```
Run 4 causes the problem - ignore it
```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(1:3,5:10)
num.it <- 750*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_Oct7_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```
Look at rmse... but it's sgld
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 750 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_Oct7_pm_sm_gpols_12init_sgld_lr99_run2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.98 ,1.25) +
    theme_minimal() +
    labs(title = paste("Test RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0, .5) +
    theme_minimal() +
    labs(title = paste("Train RMSE for", Class, " Classes"), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Plot losses
```{r}
library(ggplot2)
library(tidyr)

num.it <- 50#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_Oct7_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
I did s.d. = sqrt(learning_rate/100)

//oct8_pm_sm_gpols_12init_sgld_lr99_run2. 16050279 => changing s.d. to sqrt(learning_rate). changed number of epochs to 50

##Large jitter
Plot losses
=>run 6 went nuts
```{r}
library(ggplot2)
library(tidyr)

num.it <- 50#750 * 4
runs <- c(1:5,7:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct8_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Looking at res.mat[,1:3], I think it's decent amount of noises. Though run 3 may cause a problem
```{r}
library(coda)
library(mcmc)

# Assuming `res.mat` is a matrix with MCMC chains as rows and iterations as columns
runs <- c(2:5,7:10)
num.it <- 50*4  # Number of iterations per chain (adjust based on your actual data)
res.mat <- matrix(NA, nrow = length(runs), ncol = num.it)

# Fill res.mat with the data from the files
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct8_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Convert res.mat to mcmc.list
# Each row of res.mat represents an individual MCMC chain
mcmc_chains <- lapply(1:nrow(res.mat), function(i) {
  mcmc(res.mat[i, ])  # Convert each row (chain) to an mcmc object
})

# Combine the chains into an mcmc.list
mcmc_list <- mcmc.list(mcmc_chains)
r.hat <- tryCatch({
  gelman.diag(mcmc_list)
}, error = function(e) {
  message("Gelman-Rubin diagnostic failed: ", e$message)
  NULL
})
print(r.hat$psrf[,1])
plot(mcmc_list)

```

#oct 9
//oct9_pm_sm_gpols_12init_sgld_lr99_run2  16263656 => change initial noise to be median(abs(theta))/10
No effect really, only differ by digit  

//oct9_pm_sm_gpols_12init_sgld_lr99_run2 16265178 => change initial noise to be median(abs(theta))
//oct9_pm_sm_gpols_12init_sgld_lr99_run2 16280748 => change initial noise to be median(abs(theta))^2
##Data jitter
Plot losses
=>run 6 went nuts
```{r}
library(ggplot2)
library(tidyr)

num.it <- 10#750 * 4
runs <- c(1:5,7:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct9_pm_sm_gpols_12init_sgld_lr99_run2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Still look decent


##Jian wanna see update 
I shop plot the loss, but need to re-run first
###RMSE
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 100#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
We want to get some learning rate and get map

Running
Created `pm_sm_gpols_12init_K2.R` to correct the map and change learning of algo to 0.99/num.it
//oct9_pm_sm_gpols_12init K2 16286693


#10 oct

##loss of...
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 100#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct9_pm_sm_gpols_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
There seems to be no learning

Let's try learning rate of 1e-4, im afraid it wont work either.
//oct10_pm_sm_gpols_12init  16356966
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 10#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct10_pm_sm_gpols_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
still no learning

##Look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct10_pm_sm_gpols_12init_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Loss has weird update.



Maybe the GP-Linear really doesn't work

let's see gpgp with dec learning rate
It's either the GP transform first layer don't work at all
OR
It's not applicable to pair matching

created
`pm_sm_gpgp_12init_K2.R` => GPGP with decreasing lr
//oct11_pm_sm_gpgp_12init  16738874

#11 Oct

##GPGP
Loss
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct11_pm_sm_gpgp_12init_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
loss rose so sharply

RMSE
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500*4#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct11_pm_sm_gpgp_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct11_pm_sm_gpgp_12init_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
this is exact same plot as rising loss
      map.train <- c(map.train,n.train/2*log(y.sigma) +1/(2*y.sigma)*n.train*mseCpp(hs_in.pred_SOI,lognum[mini.batch$train[[b]]]) +l.expan/2*log(y.sigma) + 1/(2*y.sigma)*(1/1)*sum((hs_fit_SOI$post_mean$betacoef[2:(l.expan+1)])^2) + 1/2*l.bias^2)
```{r}
2000/2*log(200000) #this is 12k
1/(2*200000) #this is 2.5e-6
```

I can actually see which component is large



seems like there is no learning.
**I think the loss of gpgp currently doesn't contain first level loss, but it's still huge. let's assess this

I think 2 things to assess
1. does gpnn learn in this?
  //oct12_pm_sm_gpnn_12init 16887303
2. Does gpgp or gp ols also not learn in age prediction 
  2.1 actually first let's look at the plot of log(sigma) and 1/2sigma which i already
      let's also try separating Posterior into Likelihood and Prior
      Also incorporate likelihood and prior in `pm_sm_gpgp_12init_K2.R`
      //oct12_pm_sm_gpgp_12init 16890906 => 50 epochs with likelihood and prior


##GPNN
Loss
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
it's huge
look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 15#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
So sigma is only 6k but loss is 8 million.

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 100#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
No learning isible for this either. But in bbs rmse should get to 0? 
Let me do it with higher lr


##GPGP
look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at likelihood
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_K2_likelihood__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at prior
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_K2_prior__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
seems like it's all updating the other way. likelihood should be small tho..

3 things to try about this
1. change the sign of update
    //oct12_pm_sm_gpgp_12init_sign_switch 16896004
2. cap y.sigma
3. turn y.sigma into estimation rather than gradient update => either prediction or keep using the estimated train (ie no update)
    //oct12_pm_sm_gpgp_12init_sigma_fixed 16895954

#13 Oct

##GPNN
GPNN with higher learning rate
oct13_pm_sm_gpnn_12init  16896857 ==> lr = 0.99


##GPGP change sign
look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sign_switch_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sign_switch_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at likelihood
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sign_switch_K2_likelihood__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at prior
```{r}
library(ggplot2)
library(tidyr)

num.it <- 5#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sign_switch_K2_prior__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
nothing is showing? It's just NA.
I think it comes from the fact that sigma is negative

look at rmse
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 100#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sign_switch_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
So rmse is still good even with *WRONG* side of update?

In this case, I am guessing it must be the final layer that dominates this learning.
**Very important **

##GPGP fixed sigma
look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 200#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sigma_fixed_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 200#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sigma_fixed_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at likelihood
```{r}
library(ggplot2)
library(tidyr)

num.it <- 200#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sigma_fixed_K2_likelihood__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at prior
```{r}
library(ggplot2)
library(tidyr)

num.it <- 200#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sigma_fixed_K2_prior__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at rmse
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpgp_12init_sigma_fixed_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

Here RMSE is fine as well.

##Conclusion so far is 
1. the architecture doesn't work well regardless of right or wrong update
  1.1 some runs still seem better than others?
2. Most likely pm task may be too difficult

Let's go back to modelling age to check if loss and RMSE is decreasing
Created 
`age_sm_gpnn_12init_K2.R` oct13_age_sm_gpnn_12init 17096057 => i accidentally saved it to `oct12_pm_sm_gpnn_12init` with 500 epochs..
`age_sm_gpgp_12init_K2.R` oct13_age_sm_gpgp_12init 17096077


#14 Oct

##Look at modelling age 

###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 400#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

Update only happen like 1-2 iteations then stuck at much lower than s.d., but the "much lower" is context-based. 
I will run it again
//oct14_age_sm_gpnn_12init 17100372 => 50 epochs, lr = 1e-2 and decreasing

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 100#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_age_sm_gpnn_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    # ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
This is saying that learning rate is too small.

How about fixing lr to not be decreasing at all let's try lr = 0.3 and fix that
//oct14_age_sm_gpnn_12init_fixlr 17167270

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_age_sm_gpnn_12init_fixlr_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    # ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    # ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Here works fine!!!!


For lr = 0.99
look at losses
look at sigma
```{r}
library(ggplot2)
library(tidyr)

num.it <- 100#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K2_sigma__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 100#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct12_pm_sm_gpnn_12init_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```


###GPGP
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 400#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct13_age_sm_gpgp_12init_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.8, 15) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.8, 8) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
we kinda see learning here?
since gpnn, age modelling using fixed lr, work, let's try here.

//oct14_age_sm_gpgp_12init_fixlr  17169779
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_age_sm_gpgp_12init_fixlr_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(4, 15) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(4, 8) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
so yes, there is learning, up until certain points before it gets worse


so let's run gp-ols like this?

oct14_age_sm_gpols_12init_fixlr3e1 17370798
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_age_sm_gpols_12init_fixlr3e1_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(4, 8) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    # geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    # geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$age[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(4, 8) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```





##Pair Matching
//oct14_pm_sm_gpnn_12init_fixlr 17171909 => lr is fixed at 0.1
//oct14_pm_sm_gpols_12init_fixlr 17176320 => lr is fixed at 0.1
//oct14_pm_sm_gpgp_12init_fixlr 17176554 => lr is fixed at 0.3

###GPGP
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:8)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpgp_12init_fixlr_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
let's try reducing lr

###GP-OLS
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:8)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```


###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:8)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
so maybe learning rate should be smaller?

//oct14_pm_sm_gpnn_12init_fixlr1e2 17178654 => change learning rate from 0.1 to 0.01
//oct14_pm_sm_gpgp_12init_fixlr1e2  17181573 => change learning rate from 0.3 to 0.01
//oct14_pm_sm_gpols_12init_fixlr1e2 17181583 => change learning rate from 0.1 to 0.01

###GPGP
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpgp_12init_fixlr1e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr1e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Cannot tell


###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:8)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr1e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Maybe lr too small?
//oct14_pm_sm_gpnn_12init_fixlr6e2 17181699

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:8)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr6e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

let's do 3e2 for both gpgp and gpnn 
oct14_pm_sm_gpnn_12init_fixlr3e2 17192914
//oct14_pm_sm_gpgp_12init_fixlr3e2 17192916
//oct14_pm_sm_gpols_12init_fixlr3e2 17194188

```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```
###GPGP
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpgp_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200  # Number of iterations
runs <- c(1:10)

for(Class in num.class) {

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs) {
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpgp_12init_fixlr1e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the test data (res.mat[2,,])
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(resdat_test$Iteration), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot

  ### Train RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the train data (res.mat[1,,])
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(resdat_train$Iteration), by = 10)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Learning is visible at 1e2, on test data here.


###GPOLS
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 400#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 400#500 * 4
runs <- c(2,9,10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 40)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

Maybe it needs to be run for longer
//oct14_pm_sm_gpols_12init_fixlr3e2  17201155

####25th percentile
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 400  # Number of iterations
runs <- c(1:10)

for(Class in num.class) {
  
  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the test data (res.mat[2,,])
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(resdat_test$Iteration), by = 40)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot

  ### Train RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the train data (res.mat[1,,])
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(resdat_train$Iteration), by = 40)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
look at map
```{r}
library(ggplot2)
library(tidyr)

num.it <- 200#500*4#750 * 4
runs <- c(1:10)
res.mat <- matrix(,nrow = length(runs), ncol = num.it)
for(i in runs){
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr3e2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE
resdat <- as.data.frame(res.mat)#[,1:5]  # Use second row of res.mat (Test RMSE)
colnames(resdat) <- 1:ncol(resdat)
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  # scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```

```{r}
library(ggplot2)
library(tidyr)

num.it <- 400  # Total number of iterations
runs <- c(1:10)
res.mat <- matrix(, nrow = length(runs), ncol = num.it)

for (i in runs) {
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr3e2_K2_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE for iterations 10 to 400
resdat <- as.data.frame(res.mat)[, 10:400]  # Use iterations 10 to 400
colnames(resdat) <- 10:400  # Update column names to reflect the actual iteration numbers
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(10, 400, by = 50), limits = c(10, 400)) +  # Set x-axis from 10 to 400 with breaks every 50
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```



###GPNN
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200#500 * 4
runs <- c(1:4)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 10)) +
    # ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Effect of overfitting can somewhat be observed

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 200  # Number of iterations
runs <- c(1:4)

for(Class in num.class) {

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs) {
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpnn_12init_fixlr3e2_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the test data (res.mat[2,,])
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, num.it, by = 10)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot

  ### Train RMSE ###
  # Calculate the 25th percentile for each column (iteration) for the train data (res.mat[1,,])
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0.25))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, num.it, by = 10)) +
    theme_minimal() +
    labs(title = paste("Train RMSE (25th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "25th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

//oct14_pm_sm_gpols_12init_fixlr0. 17205167


What happened if i put learning rate = 0? test rmse should be flat and training should be somewhat random from random minibatches


#15 Oct

##lr = 0

###GPOLS
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 12#500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct14_pm_sm_gpols_12init_fixlr0_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 4)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 4)) +
    ylim(0.89, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
It's not a cycle.
==> I think it's because the last layer is not gradient-update, but rather model fitting. So changing minibatches data will change this model fitting, leading to different results.


##For looking at losses, it should be observed after a few burn-ins

##Observe predictions vs true for pm_gp => do this for bbs
###gpols bbs
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  # Test RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[2,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  test_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.99, 1.1) +
    theme_minimal() +
    labs(title = paste("Test RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(test_plot)  # Print the Test RMSE plot
  
  # Train RMSE plot
  resdat <- as.data.frame(sqrt(res.mat[1,,]))
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))
  resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")
  
  train_plot <- ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
    geom_line() +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, max(as.numeric(resdat_long$column)), by = 500)) +
    ylim(0.88, 1.1) +
    theme_minimal() +
    labs(title = paste("Train RMSE for Class", Class), x = "Iterations", y = "Value", color = "Run number")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Run 2 achieved lowest.

```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(1.03, 1.07) +
    theme_minimal() +
    labs(title = paste("Test RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(0.88, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Locating the minimum
```{r}
which.min(resdat_test[,2])
resdat_test[798,2]
```
Lowest point here is the same as Run 2.


Let's look at the class of K = 2
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

```{r}
library(dplyr)
library(ggplot2)
library(feather)
library(patchwork)  # For combining plots

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (age.in$pm_tf - dat.in$pred), as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'residual', 'class')

  ### 1. Scatter Plot: pm_tf (Truth) vs pred (Fitted)
  truth_vs_fitted_plot <- ggplot(age.in, aes(x = pred, y = pm_tf, color = class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # Add a y=x line
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred")) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Fitted vs Truth", x = "Fitted Values (pred)", y = "True Values (pm_tf)", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### 2. Scatter Plot: SqError vs pred (Fitted)
  sqerror_vs_fitted_plot <- ggplot(age.in, aes(x = pred, y = residual, color = class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred")) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Residual vs Fitted", x = "Fitted Values (pred)", y = "y-y^", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### Combine the two plots using patchwork
  combined_plot <- truth_vs_fitted_plot + sqerror_vs_fitted_plot + 
    plot_layout(ncol = 2) +  # Arrange the two plots side by side
    plot_annotation(title = paste("Scatter Plots for Run", r))  # Add a main title for the combined plot

  print(combined_plot)
}
```



#17 Oct

##GP-OLS results
Look at R^2
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,1],c(0.5)))
(l.test <-quantile(hs.int[,3],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,1],c(0.5)))
(r.test <-quantile(hs.int[,3],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,1],c(0.5)))
(gpr.test <-quantile(hs.int[,3],c(0.5)))
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K", Class, "_rsq__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame((res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply((res.mat[2,,]), 2, function(x) quantile(x, 1))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(-1, 6) +
    theme_minimal() +
    labs(title = paste("Test R^2 (100th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "100th Percentile R^2", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame((res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply((res.mat[1,,]), 2, function(x) quantile(x, 1))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(-1, 20) +
    theme_minimal() +
    labs(title = paste("Train R^2 (100th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "100th Percentile R^2", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
Locating the max r-squared
```{r}
which.max(resdat_test[,2])
resdat_test[798,2]
```


#21 Oct

## Increasing held-out samples
Expanding held-out to assess the preditability of our model (maybe the model only perform well on the stratified held-out data)
to do
1. Record the best params on the validation set and use those to make predictions ==> note that this mean I also have to record the hidden layer params, otherwise fitting models on full data will be different.
  modified `pm_sm_gpols_12init_K_bbs.R` oct21_pm_sm_gpols_12init_bbs 19387987

2. Compare the above against e.g. params at not the best result (e.g. at final iteration)
  result right now include 2 class gpols, then 3 class of other algo, then bimodal and data themselves


##Summary table of everything so far
methd | best rmse (train test) | r-square | SGLD? | num param | run time

###data and linear mod
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
print(sd(age_tab$pm_tf[train.test.ind$train]))
print(sd(age_tab$pm_tf[train.test.ind$test]))
```
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
print(apply(hs.int[,c(1,3,5,6)],MARGIN=2,FUN = quantile,c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
print(apply(hs.int[,c(1,3,5,6)],MARGIN=2,FUN = quantile,c(0.5)))


#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
print(apply(hs.int[,c(1,3,5,6)],MARGIN=2,FUN = quantile,c(0.5)))

```
```{r}
# Initialize a matrix for storing the results
results <- matrix(NA, nrow = 3, ncol = 5)
colnames(results) <- c("Training RMSE", "Test RMSE" ,"Training R^2", "Test R^2","non-zero Var")
rownames(results) <- c("Lasso", "Ridge", "GPR")

# Lasso Results
runs.hs <- 1:10
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_", i, ".csv"))))
}
lasso_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["Lasso", ] <- lasso_quantiles

# Ridge Results
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_", i, ".csv"))))
}
ridge_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["Ridge", ] <- ridge_quantiles

# GPR Results
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_", i, ".csv"))))
}
gpr_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["GPR", ] <- gpr_quantiles

# Convert to a data frame and print the table
results_df <- as.data.frame(results)
results_df$Method <- rownames(results_df)

# Reorder columns to put "Method" as the first column
results_df <- results_df[, c("Method", "Training RMSE", "Test RMSE", "Training R^2", "Test R^2","non-zero Var")]

# Print the table
print(results_df)
```

###GP-ols K2
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```

###GP-ols K2 at convergence
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, ,num.it]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse))  # Find location of minimum MSE

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, min_mse_idx, num.it])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, min_mse_idx, num.it])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, min_mse_idx, num.it]  # Training R^2
test_rsq <- res.mat[4, min_mse_idx, num.it]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = min_mse_idx,
  Iteration = num.it,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```
```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_theta__jobid_1.feather")))
co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_bias__jobid_1.csv"))$x
co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_cobias__jobid_1.csv"))$x
l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(co.weights) + length(bias) + length(co.bias) + length(l.theta) + 1
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```


###GP-ols K3
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```
```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_theta__jobid_1.feather")))
co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_bias__jobid_1.csv"))$x
co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_cobias__jobid_1.csv"))$x
l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(co.weights) + length(bias) + length(co.bias) + length(l.theta) + 1
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```
###GP-ols K4 or K8
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
print(which.min(res.mat[2,,2000]))
```

```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_theta__jobid_1.feather")))
co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_bias__jobid_1.csv"))$x
co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_cobias__jobid_1.csv"))$x
l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(co.weights) + length(bias) + length(co.bias) + length(l.theta) + 1
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```

###GP-NN K3
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1,2,4,6,10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```

```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_weights__jobid_1.feather")))
co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_bias__jobid_1.csv"))$x
co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_cobias__jobid_1.csv"))$x
# l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(co.weights) + length(bias) + length(co.bias) + 1 + 5456 + 16368
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```

###GP-GP K3
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:8,10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```

```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_theta__jobid_1.feather")))
co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_bias__jobid_1.csv"))$x
co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_cobias__jobid_1.csv"))$x
# l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(co.weights) + length(bias) + length(co.bias) + 1 + 5456 + 16368
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```

###GP-ols no covariates
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```

```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_theta__jobid_1.feather")))
# co.weights<- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_coweights__jobid_1.csv")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_bias__jobid_1.csv"))$x
# co.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_cobias__jobid_1.csv"))$x
l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_lweights__jobid_1.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix) + length(bias) + length(l.theta) + 1
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```

###bimodal linear mod
*** note I needa check whether the train-test datasets are similar, I bet there is a difference but not sure how small. Also check if test data was properly transformed here.
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/age_dmn_sex_strat.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_dmn_train_index.csv')$x
print(sd(age_tab$pm_tf[train.test.ind$train]))
print(sd(age_tab$pm_tf[train.test.ind$test]))
```

seems wrong - seems like test was used to perform with training. But I thought I corrected it. I think it's fixed. We can see the plotted distributions of train and test for both bimodal and non-bimodal

```{r}
# Initialize a matrix for storing the results
results <- matrix(NA, nrow = 3, ncol = 5)
colnames(results) <- c("Training RMSE", "Test RMSE" ,"Training R^2", "Test R^2","non-zero Var")
rownames(results) <- c("Lasso", "Ridge", "GPR")

# Lasso Results
runs.hs <- 1:10
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_lasso_noscale_", i, ".csv"))))
}
lasso_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["Lasso", ] <- lasso_quantiles

# Ridge Results
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_ridge_noscale_", i, ".csv"))))
}
ridge_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["Ridge", ] <- ridge_quantiles

# GPR Results
hs.int <- matrix(, nrow = length(runs.hs), ncol = 13)
for(i in runs.hs){
  hs.int[which(i == runs.hs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug13_bimodal_pm_gpr_noscale_", i, ".csv"))))
}
gpr_quantiles <- apply(hs.int[, c(5, 6,1,3,13)], MARGIN = 2, FUN = quantile, c(0.5))
results["GPR", ] <- gpr_quantiles

# Convert to a data frame and print the table
results_df <- as.data.frame(results)
results_df$Method <- rownames(results_df)

# Reorder columns to put "Method" as the first column
results_df <- results_df[, c("Method", "Training RMSE", "Test RMSE", "Training R^2", "Test R^2","non-zero Var")]

# Print the table
print(results_df)
```

###bimodal gpnn


12 regions too
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 250 * 4
runs <- c(3,4,10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpnn_init_bbs_loss__jobid_",i,".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpnn_init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```
 
###bimodal gp-ols
This currently assume elementwise interaction of hidden-hidden which is not exhuastive enough.
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 250 * 4
runs <- c(2,6,7,9)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_loss__jobid_",i,".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)

# Print the results dataframe
print(results_df)
```
```{r}
theta.matrix <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_theta__jobid_2.feather")))
bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_bias__jobid_2.csv"))$x
l.theta <- as.matrix(read_feather(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_bi_gpols_init_lweights__jobid_2.feather")))
#+1 for l.bias
# Calculate the total number of elements across all variables
total_variables <- length(theta.matrix)*2 + length(bias)*2 + length(l.theta) + 1
# Print the total number of variables
cat("Total number of variables:", total_variables, "\n")
```


#23 Oct
Need to count number of params and run time.

Number of params equal to size of weights/theta + bias + coweights + cobias + lweight (lweight is pre/post transformed?, should be pre) + lbias (1)
Need to be cause of concat weight for gpnn due to post-transformation save. 

##Look at number of param
We did it above


#25 Oct
notes from before:
1. Record the best params on the validation set and use those to make predictions ==> note that this mean I also have to record the hidden layer params, otherwise fitting models on full data will be different.
  modified //`pm_sm_gpols_12init_K_bbs.R` oct25_pm_sm_gpols_12init_bbs 19387987

2. Compare the above against e.g. params at not the best result (e.g. at final iteration)
  result right now include 2 class gpols, then 3 class of other algo, then bimodal and data themselves



##Plot new bbs
```{r}
library(ggplot2)
library(tidyr)

num.it <- 250 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct25_pm_sm_gpols_12init_bbs_K2_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(1.03, 1.07) +
    theme_minimal() +
    labs(title = paste("Test RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(0.88, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
All good, I think lowest point achieved at same place.


##To do
- Do test predictions based on these on bigger dataset
  - current, the data has 5204 participants, meaning i can do 1200 more right away.
- look at ridge result, split data into halves and come up with a way to 

##Compute transform_tf for the rest of data
Original data
```{r}
library(feather)
age_tab2 <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
Transformation
```{r}
# Load necessary packages
if (!require(scales)) install.packages("scales")
if (!require(caret)) install.packages("caret")
library(scales)
library(caret)
library(ggplot2)

# Assuming age_tab and train.test.ind are already defined with the response variable called 'pm'

# Function to perform Gaussian Inverse Transformation on training data
gaussian_inverse_transform_train <- function(response_variable) {
  # Rank transformation
  ranks <- rank(response_variable)
  
  # Normal quantile transformation
  normal_quantiles <- qnorm((ranks - 0.5) / length(ranks))
  
  return(list(
    transformed = normal_quantiles,
    ecdf_fn = ecdf(response_variable)  # Empirical CDF function for the training data
  ))
}

# Apply pre-learned transformation to new data
apply_prelearned_transformation <- function(test_response_variable, ecdf_fn) {
  # Apply the ECDF to the test data
  ecdf_values <- ecdf_fn(test_response_variable)
  
  # Replace exact 0 and 1 values
  ecdf_values[ecdf_values == 0] <- .Machine$double.eps
  ecdf_values[ecdf_values == 1] <- 1 - .Machine$double.eps
  
  # Normal quantile transformation using the ECDF values
  normal_quantiles <- qnorm(ecdf_values)
  
  return(normal_quantiles)
}

# Learn the transformation on the training set
train_indices <- train.test.ind$train
test_indices <- setdiff(seq(nrow(age_tab)),train.test.ind$train)

train_lognum <- age_tab$pm[train_indices]

# Apply the Gaussian Inverse Transformation on the training set
train_transformation <- gaussian_inverse_transform_train(train_lognum)

# Add the transformed training values back to the dataframe
age_tab$pm_tf <- NA
age_tab$pm_tf[train_indices] <- train_transformation$transformed

# Apply the learned transformation on the test set
test_lognum <- age_tab$pm[test_indices]
test_transformed <- apply_prelearned_transformation(test_lognum, train_transformation$ecdf_fn)

# Add the transformed test values back to the dataframe
age_tab$pm_tf[test_indices] <- test_transformed

# write_feather(age_tab, '/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather')
```

Running 
Created `pm_sm_gpols_12init_K_bbs_ext.R` to make held-out prediction on 1204 unseen data.
//oct25_pm_sm_gpols_12init_bbs_K2... outpred_ext  19505288 ==> all done in 0.75 seconds.



##initialising co-weight via ridge
Load the training ridge predictions. 
Jian suggest we can separate into group with high error vs low error (but this doesn't make sense no?) OR i guess we can do (lower than mean vs higher)

idk which run to pick (ideally compute errors across different runs and average?)
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```

```{r}
# Load the ridge training predictions
ridge.train <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_inpred_noscale_", 1, ".csv")))

# Compute squared errors
squared_errors <- (age_tab$pm_tf[train.test.ind$train] - c(ridge.train))^2

# Calculate the median of squared errors
med.error <- median(squared_errors)

# Get indices where squared error < median
indices_below_median <- which(squared_errors < med.error)

# Get indices where squared error >= median
indices_above_or_equal_median <- which(squared_errors >= med.error)

##True dataset index is
ind.below <- train.test.ind$train[indices_below_median]
ind.above <- train.test.ind$train[indices_above_or_equal_median]

#find the exact combination to yield what we look for. I think it turns into an SVM problem (non-linear)
#so we create a synthetic response of group 1 and 2 as an outcome of softmax, then reverse engineer to find the right weights for co-weights
```
Load up co.dat
```{r}
  age <- as.numeric(age_tab$age)
  sex <-  as.numeric(age_tab$sex)
  sex <- sapply(sex, function(x) replace(x, x==0,-1)) #Change female to -1, male to 1
  
  train.test.ind <- list()
  train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
  train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
  n.train <- length(train.test.ind$train)
  
  depind <- age_tab$DepInd
  quantile_thresholds <- quantile(depind[train.test.ind$train], probs = seq(0, 1, by = 0.34))
  #age.group <- ifelse(age > mean(age), yes = 1, no = -1)
  dep.group1 <- ifelse(depind <=quantile_thresholds[[2]], yes =1, no = 0)
  dep.group2 <- ifelse(depind > quantile_thresholds[[2]] & depind <=quantile_thresholds[[3]], yes =1, no = 0)
  dep.group3 <- ifelse(depind > quantile_thresholds[[3]], yes =1, no = 0)
  
  quantile_thresholds <- quantile(age[train.test.ind$train], probs = seq(0, 1, by = 0.34))
  #age.group <- ifelse(age > mean(age), yes = 1, no = -1)
  age.group1 <- ifelse(age <=quantile_thresholds[[2]], yes =1, no = 0)
  age.group2 <- ifelse(age > quantile_thresholds[[2]] & age <=quantile_thresholds[[3]], yes =1, no = 0)
  age.group3 <- ifelse(age > quantile_thresholds[[3]], yes =1, no = 0)
  
  co.dat <- as.data.frame(cbind(sex,dep.group1,dep.group2,dep.group3 ,age.group1,age.group2,age.group3))
  co.dat <- co.dat[train.test.ind$train ,]
```

```{r}
# Set up data
# Initialize Data
co.dat$response <- ifelse(train.test.ind$train %in% ind.below, 1, 2)  # Assign labels "1" and "2"
# Load the required package
if (!require(nnet)) install.packages("nnet", dependencies = TRUE)
library(nnet)

# Prepare data with more than two classes in the response variable
# Example: assuming `co.dat$response` has more than two classes
co.dat$response <- as.factor(co.dat$response)  # Ensure the response variable is a factor for multiclass

# Fit the multinomial logistic regression model
model <- multinom(response ~ ., data = co.dat)

# View the model summary to understand the coefficients for each class
summary(model)

# Predict on the training data
pred_classes <- predict(model, co.dat)

# Calculate accuracy
accuracy <- mean(pred_classes == co.dat$response) * 100

# Print the results
cat("Multiclass Training Accuracy:", round(accuracy, 2), "%\n")
```
```{r}
# Load the required package
if (!require(nnet)) install.packages("nnet", dependencies = TRUE)
library(nnet)

# Prepare the response variable with two classes
co.dat$response <- as.factor(ifelse(train.test.ind$train %in% ind.below, 1, 2))  # Convert to factor

# Fit the multinomial logistic regression model with multinom()
model <- multinom(response ~ ., data = co.dat)

# Extract coefficients for each class relative to the reference (class 2)
coefficients <- coef(model)

# Convert coefficients into a (2x7) matrix for weights and a (2) vector for biases
weights <- rbind(0, coefficients[-1])  # Add zero for the reference class weights
biases <- c(0, coefficients[1])        # Add zero for the reference class bias

## because of the binary set-up, this multinom only outputs one set of weights. So we can do a refernece class?


# Print weights and biases
cat("Weights (2x7 matrix):\n")
print(weights)
cat("\nBiases (2-element vector):\n")
print(biases)

# Calculate softmax probabilities using weights and biases
softmax_probs <- function(X, weights, biases) {
  Z <- X %*% t(weights) + biases  # Linear combination for each class
  exp_Z <- exp(Z - apply(Z, 1, max))  # Subtract max for numerical stability
  exp_Z / rowSums(exp_Z)
}

# Example prediction using softmax probabilities on training data
X <- as.matrix(co.dat[, -ncol(co.dat)])  # Exclude the response column
pred_probs <- softmax_probs(X, weights, biases) #but this is not what i wanted

# Print the probabilities for each class
head(pred_probs)
```

Created `pm_sm_gpols_guidedinit_K.R`
//oct26_pm_sm_gpols_guidedinit 19510183 19562197


#28 Oct

##Evaluating gpols K = 2 on unseen
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x

train.test.ind$unseen <- setdiff(seq(nrow(age_tab)),c(train.test.ind$train,train.test.ind$test))
```
```{r}
#data sd
sd(age_tab$pm_tf[train.test.ind$unseen])
var(age_tab$pm_tf[train.test.ind$test])

```
Loading in data
/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct25_pm_sm_gpols_12init_bbs_K2_outpred_ext__jobid_1.feather
```{r}
#3 x 1204 where first row is index starting from 1, but unseen starts form 3 goes up to 5xxx, but all ordered
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct25_pm_sm_gpols_12init_bbs_K2_outpred_ext__jobid_1.feather'))
out.pred[2,] <- out.pred[2,] - mean(out.pred[2,])
#calculating rmse
sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,]))^2)) #somehow this predict everything to -833, I think the intercept is -833 
```
This is only marginally better... let me assess

###Loading lbias and lweights
```{r}
l.weights <- c(unlist(read_feather(paste0( "/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct25_pm_sm_gpols_12init_bbs_K2","_lweights__jobid_",2,'.feather'))))
l.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct25_pm_sm_gpols_12init_bbs_K2","_minlbias__jobid_",2,".csv"))$x
```

Running
//`oct25_pm_sm_gpols_12init_bbs_K2 ... _outpred_ext_test_` to assess all predictions 19562032
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct25_pm_sm_gpols_12init_bbs_K2_outpred_ext_test__jobid_1.feather'))

sd(age_tab$pm_tf[train.test.ind$train])
sd(age_tab$pm_tf[train.test.ind$test])
sd(age_tab$pm_tf[train.test.ind$unseen])


out.pred[2,train.test.ind$train] <- out.pred[2,train.test.ind$train] - mean(out.pred[2,train.test.ind$train])
out.pred[2,train.test.ind$test] <- out.pred[2,train.test.ind$test] - mean(out.pred[2,train.test.ind$test])
out.pred[2,train.test.ind$unseen] <- out.pred[2,train.test.ind$unseen] - mean(out.pred[2,train.test.ind$unseen])

sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)) #somehow this predict everything to -833, I think the intercept is -833 
sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)) #somehow this predict everything to -833, I think the intercept is -833 
sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)) #somehow this predict everything to -833, I think the intercept is -833 

```
Something is very off.
//oct28_pm_sm_gpols_12init_bbs 19562125 => to assess the saved l.weights and l.bias ... l.bias is still huge... let's look at mean prediction?
//oct28_pm_sm_gpols_12init_bbs 19564852 => assessing the summary of predictions and rmse ==> so oct25 had the right test rmse. but right now what's wrong is the prediction


running
//oct26_pm_sm_gpols_guidedinit_bbs 19565485


##Identifying lowest point of oct28_pm_sm_gpols_12init_bbs
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 250 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(1.03, 1.07) +
    theme_minimal() +
    labs(title = paste("Test RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(0.88, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
locating run and iteration of min
```{r}
resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
print(which(as.matrix(resdat) == min(as.matrix(resdat)), arr.ind=TRUE))
```

Locating the minimum
```{r}
which.min(resdat_test[,2])
resdat_test[798,2]
```
let's read in this
```{r}
# for(i in 1:10){
  print(sqrt(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K2_minloss__jobid_",2,".csv"))))
# }
```
Okay it does coincide, just need to sqrt() it.
###Loading lbias and lweights
```{r}
l.weights <- c(unlist(read_feather(paste0( "/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct28_pm_sm_gpols_12init_bbs_K2","_minlweights__jobid_",2,'.feather'))))
l.bias <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct28_pm_sm_gpols_12init_bbs_K2","_minlbias__jobid_",2,".csv"))$x
```
Okay right now the saved lweights and the displayed are not correct. They got the same 0, but all non-zero are totally wrong. Maybe Feather messes up this?
//oct28_pm_sm_gpols_12init_bbs_K2 19608264 ==> saving lweights as csv and feather

```{r}
l.weights <- c(unlist(read_feather(paste0( "/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct28_pm_sm_gpols_12init_bbs_K2","_lweights__jobid_",2,'.feather'))))
l.weights2 <- read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_","oct28_pm_sm_gpols_12init_bbs_K2","_minlweightsCSV__jobid_",2,".csv"))
```

It's all the same... i mistakenly used lweights instead of minlweights....

##Guide gpols
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
```
###Ridge
```{r}
#lasso
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_noscale_",i,".csv"))))
}
(l.train <- quantile(hs.int[,5],c(0.5)))
(l.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_noscale_",i,".csv"))))
}
(r.train <- quantile(hs.int[,5],c(0.5)))
(r.test <-quantile(hs.int[,6],c(0.5)))

#ridge
runs.hs <- 1:10
hs.int<-matrix(,nrow=length(runs.hs),ncol=13)
for(i in runs.hs){
  hs.int[which(i==runs.hs),] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_noscale_",i,".csv"))))
}
(gpr.train <- quantile(hs.int[,5],c(0.5)))
(gpr.test <-quantile(hs.int[,6],c(0.5)))
```
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 250 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct26_pm_sm_gpols_guidedinit_bbs_K", Class, "_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(1.03, 1.07) +
    theme_minimal() +
    labs(title = paste("Test RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    ylim(0.88, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
locating run and iteration of min
```{r}
resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
print(which(as.matrix(resdat) == min(as.matrix(resdat)), arr.ind=TRUE))
```
What?? exact same run and iter? Graphs dont coincide but how coincidental??

Need to look at the classes
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct26_pm_sm_gpols_guidedinit_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

So this is after optimisation. Need to look at initial iteration for class ==> i need to look at init not bbs ==> cannot since I didn't save class prediction during param search
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct26_pm_sm_gpols_guidedinit_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- head(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'pred', 'SqError', 'class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(class) %>%
    summarise(median_pred = median(pred, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(class)
  
  # Reorder the `class` factor based on the median `pred`
  age.in$class <- factor(age.in$class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pred, SqError, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
}
```

//`oct28_pm_sm_gpols_12init_bbs_K2 ... _outpred_ext_test_` to assess all predictions 19925689, 19925789... 19941461 ==> specifically specify run 2

#29 Oct

##Evaluating gpols K = 2 on unseen
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x

train.test.ind$unseen <- setdiff(seq(nrow(age_tab)),c(train.test.ind$train,train.test.ind$test))

#data sd
sd(age_tab$pm_tf[train.test.ind$unseen])
```
Loading data
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K2_min_outpred_ext_test__jobid_1.feather'))

sd(age_tab$pm_tf[train.test.ind$train])
sd(age_tab$pm_tf[train.test.ind$test])
sd(age_tab$pm_tf[train.test.ind$unseen])

sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)) 
sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2))  #this should be 1.033
sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2))

```

```{r}
# Load predictions and class assignments
out.pred <- as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K2_min_outpred_ext_test__jobid_1.feather'))

# Define a function to calculate RMSE for each class within a given index set
calculate_rmse_by_class <- function(indices, class_labels, true_values, predictions) {
  unique_classes <- unique(class_labels[indices])
  rmse_by_class <- sapply(unique_classes, function(class) {
    class_indices <- indices[class_labels[indices] == class]
    sqrt(mean((true_values[class_indices] - predictions[class_indices])^2))
  })
  names(rmse_by_class) <- paste0("Class_", unique_classes)
  return(rmse_by_class)
}

# True values and predictions
true_values <- age_tab$pm_tf
predictions <- out.pred[2, ]  # Prediction row
class_labels <- out.pred[3, ]  # Class assignment row

# Calculate RMSE for each class within training, test, and unseen indices
train_rmse_by_class <- calculate_rmse_by_class(train.test.ind$train, class_labels, true_values, predictions)
test_rmse_by_class <- calculate_rmse_by_class(train.test.ind$test, class_labels, true_values, predictions)
unseen_rmse_by_class <- calculate_rmse_by_class(train.test.ind$unseen, class_labels, true_values, predictions)

# Display results
cat("Training RMSE by Class:\n")
print(train_rmse_by_class)
cat("\nTest RMSE by Class:\n")
print(test_rmse_by_class)
cat("\nUnseen RMSE by Class:\n")
print(unseen_rmse_by_class)
```

All good. Now do this for all??




##Bi-GP-OLS
Fixed the interactions to span across all hidden layers `pm_bi_gpols_init.R`
oct29_pm_bi_gpols_init 19855223 ==> all died after 2 iterations, very strange.


Running 
//aug22_pm_sm_gpols_12init_bbs_K2  _min_outpred_ext_test_ 19896030

#30 oct

##look at GPOLS K = 2 at convergence
NOTE: I accidentally over-wrote this on the minimum results
Re-runninb 
aug22_pm_sm_gpols_12init_bbs_K2 _outpred_ext_test_ 19920610
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x

train.test.ind$unseen <- setdiff(seq(nrow(age_tab)),c(train.test.ind$train,train.test.ind$test))

#data sd
sd(age_tab$pm_tf[train.test.ind$unseen])
```
Loading data
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_outpred_ext_test__jobid_1.feather'))

sd(age_tab$pm_tf[train.test.ind$train])
sd(age_tab$pm_tf[train.test.ind$test])
sd(age_tab$pm_tf[train.test.ind$unseen])

sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)) 
sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2))  #this should be 1.033
sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2))

```
```{r}
# Load predictions and class assignments
out.pred <- as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_outpred_ext_test__jobid_1.feather'))

# Define a function to calculate RMSE for each class within a given index set
calculate_rmse_by_class <- function(indices, class_labels, true_values, predictions) {
  unique_classes <- unique(class_labels[indices])
  rmse_by_class <- sapply(unique_classes, function(class) {
    class_indices <- indices[class_labels[indices] == class]
    sqrt(mean((true_values[class_indices] - predictions[class_indices])^2))
  })
  names(rmse_by_class) <- paste0("Class_", unique_classes)
  return(rmse_by_class)
}

# True values and predictions
true_values <- age_tab$pm_tf
predictions <- out.pred[2, ]  # Prediction row
class_labels <- out.pred[3, ]  # Class assignment row

# Calculate RMSE for each class within training, test, and unseen indices
train_rmse_by_class <- calculate_rmse_by_class(train.test.ind$train, class_labels, true_values, predictions)
test_rmse_by_class <- calculate_rmse_by_class(train.test.ind$test, class_labels, true_values, predictions)
unseen_rmse_by_class <- calculate_rmse_by_class(train.test.ind$unseen, class_labels, true_values, predictions)

# Display results
cat("Training RMSE by Class:\n")
print(train_rmse_by_class)
cat("\nTest RMSE by Class:\n")
print(test_rmse_by_class)
cat("\nUnseen RMSE by Class:\n")
print(unseen_rmse_by_class)
```
This shows that unseen for class 1 is worst than the data itself.


Let's try K = 3, 8 and maybe gpnn?
for K = 8: //aug22_pm_sm_gpols_12init_bbs_K8 _outpred_ext_test_ 19900662
Loading data
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_outpred_ext_test__jobid_1.feather'))

sd(age_tab$pm_tf[train.test.ind$train])
sd(age_tab$pm_tf[train.test.ind$test])
sd(age_tab$pm_tf[train.test.ind$unseen])

sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)) 
sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2))  #this should be 1.033
sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2))

```
```{r}
# Load predictions and class assignments
out.pred <- as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K8_outpred_ext_test__jobid_1.feather'))

# Define a function to calculate RMSE for each class within a given index set
calculate_rmse_by_class <- function(indices, class_labels, true_values, predictions) {
  unique_classes <- unique(class_labels[indices])
  rmse_by_class <- sapply(unique_classes, function(class) {
    class_indices <- indices[class_labels[indices] == class]
    sqrt(mean((true_values[class_indices] - predictions[class_indices])^2))
  })
  names(rmse_by_class) <- paste0("Class_", unique_classes)
  return(rmse_by_class)
}

# True values and predictions
true_values <- age_tab$pm_tf
predictions <- out.pred[2, ]  # Prediction row
class_labels <- out.pred[3, ]  # Class assignment row

# Calculate RMSE for each class within training, test, and unseen indices
train_rmse_by_class <- calculate_rmse_by_class(train.test.ind$train, class_labels, true_values, predictions)
test_rmse_by_class <- calculate_rmse_by_class(train.test.ind$test, class_labels, true_values, predictions)
unseen_rmse_by_class <- calculate_rmse_by_class(train.test.ind$unseen, class_labels, true_values, predictions)

# Display results
cat("Training RMSE by Class:\n")
print(train_rmse_by_class)
cat("\nTest RMSE by Class:\n")
print(test_rmse_by_class)
cat("\nUnseen RMSE by Class:\n")
print(unseen_rmse_by_class)
```

##Inspecting guided classification
loading data
```{r}
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
#age_tab <- age_tab[order(age_tab$id),].     #DOES THIS MESS UP ORDER
lognum <- age_tab$pm_tf

age <- as.numeric(age_tab$age)
sex <-  as.numeric(age_tab$sex)
sex <- sapply(sex, function(x) replace(x, x==0,-1)) #Change female to -1, male to 1

train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
n.train <- length(train.test.ind$train)

depind <- age_tab$DepInd
quantile_thresholds <- quantile(depind[train.test.ind$train], probs = seq(0, 1, by = 0.34))
#age.group <- ifelse(age > mean(age), yes = 1, no = -1)
dep.group1 <- ifelse(depind <=quantile_thresholds[[2]], yes =1, no = 0)
dep.group2 <- ifelse(depind > quantile_thresholds[[2]] & depind <=quantile_thresholds[[3]], yes =1, no = 0)
dep.group3 <- ifelse(depind > quantile_thresholds[[3]], yes =1, no = 0)

quantile_thresholds <- quantile(age[train.test.ind$train], probs = seq(0, 1, by = 0.34))
#age.group <- ifelse(age > mean(age), yes = 1, no = -1)
age.group1 <- ifelse(age <=quantile_thresholds[[2]], yes =1, no = 0)
age.group2 <- ifelse(age > quantile_thresholds[[2]] & age <=quantile_thresholds[[3]], yes =1, no = 0)
age.group3 <- ifelse(age > quantile_thresholds[[3]], yes =1, no = 0)

co.dat <- cbind(sex,dep.group1,dep.group2,dep.group3 ,age.group1,age.group2,age.group3)
```
loading guide
```{r}
library(nnet)
      # Load the ridge training predictions
      ridge.train <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_inpred_noscale_", 1, ".csv")))
      # Compute squared errors
      squared_errors <- (age_tab$pm_tf[train.test.ind$train] - c(ridge.train))^2
      # Calculate the median of squared errors
      med.error <- median(squared_errors)
      # Get indices where squared error < median
      indices_below_median <- which(squared_errors < med.error)
      # Get indices where squared error >= median
      indices_above_or_equal_median <- which(squared_errors >= med.error)
      ##True dataset index is
      ind.below <- train.test.ind$train[indices_below_median]
      ind.above <- train.test.ind$train[indices_above_or_equal_median]
      
      co.dat.ref <- as.data.frame(co.dat[train.test.ind$train,])
      co.dat.ref$response <- ifelse(train.test.ind$train %in% ind.below, 1, 2)
      
      # Fit the multinomial logistic regression model with multinom()
      model <- multinom(response ~ ., data = co.dat.ref)
      # Extract coefficients for each class relative to the reference (class 2)
      coefficients <- coef(model)
      coefficients[is.na(coefficients)] <- 0
      # Convert coefficients into a (2x7) matrix for weights and a (2) vector for biases
      co.weights <- as.matrix(rbind(0, coefficients[-1]))  # Add zero for the reference class weights
      colnames(co.weights) <- NULL
      co.bias <- c(0, coefficients[1])        # Add zero for the reference class bias
      colnames(co.bias) <- NULL
```
Pass thru ReLU, take arg max, look at the dist 
```{r}
  softmax <-function(z){
    t(apply(z,1,function(x){
      x<-10*x
      xmax <- max(x)
      return(exp(x-max(x))/sum(exp(x-max(x))))
    }))
  }
co.pre.hidden.layer <- t(t(co.dat[train.test.ind$train, ] %*% t(co.weights)) + co.bias)
co.hidden.layer <- softmax(co.pre.hidden.layer)
class.train <- apply(co.hidden.layer,1,which.max)
```

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

age.in <- age_tab[train.test.ind$train, c('age', 'sex', 'DepInd', 'pm_tf')]
age.in <- cbind(age.in, as.factor(class.train))
colnames(age.in) <- c('age', 'sex', 'DepInd', 'pm_tf', 'class')


class_sizes <- age.in %>%
  group_by(class) %>%
  summarise(count = n())

# Create a named vector for custom labels
  class_labels <- paste0(class_sizes$class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(age, sex, DepInd, pm_tf), names_to = "variable", values_to = "value")
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = class, y = value, fill = class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Age, Sex, DepInd, tf_pm, Labeled by Class for Run"),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels)
  
  print(p)
```
  It splits by gender almost perfectly??? but eventually it goes back to splitting by everything else
  Bear in mind the accuracy of splitting is very low.
  
  
##Bimodal 
//oct29_pm_bi_gpols_init 19905248  => Assessing the lweights and z.nb

##K-GPNN 
trying K = 2
//oct30_pm_sm_gpnn_12init 19921137
```{r}
library(ggplot2)
library(tidyr)

num.class <- c(2)
num.it <- 500 * 4
runs <- c(1:10)

for(Class in num.class){

  res.mat <- array(, dim = c(2, length(runs), num.it))
  for(i in runs){
    res.mat[, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_loss__jobid_", i, ".csv")))[, 1:num.it]
  }

  ### Test RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[2,,]))  # Test RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  test_percentile_25 <- apply(sqrt(res.mat[2,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_test <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = test_percentile_25
  )
  
  # Create the test plot using the 25th percentile
  test_plot <- ggplot(resdat_test, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "blue") +
    geom_hline(aes(yintercept = r.test, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.test, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.test, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$test]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.test + 0.001, label = paste("Ridge:", round(r.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.test + 0.001, label = paste("Lasso:", round(l.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.test + 0.001, label = paste("GPR:", round(gpr.test, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$test]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$test]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    # ylim(1.03, 1.07) +
    theme_minimal() +
    labs(title = paste("Test RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(test_plot)  # Print the Test RMSE plot
  
  ### Train RMSE ###
  resdat <- as.data.frame(sqrt(res.mat[1,,]))  # Train RMSE
  colnames(resdat) <- 1:ncol(resdat)
  resdat$row_id <- seq_len(nrow(resdat))

  # Calculate the 25th percentile for each column
  train_percentile_25 <- apply(sqrt(res.mat[1,,]), 2, function(x) quantile(x, 0))
  
  # Prepare the data for plotting
  resdat_train <- data.frame(
    Iteration = 1:num.it,
    Percentile25 = train_percentile_25
  )
  
  # Create the train plot using the 25th percentile
  train_plot <- ggplot(resdat_train, aes(x = Iteration, y = Percentile25)) +
    geom_line(color = "red") +
    geom_hline(aes(yintercept = r.train, color = "Ridge"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = l.train, color = "Lasso"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = gpr.train, color = "GPR"), linetype = "dashed", size = 1) +
    geom_hline(aes(yintercept = sd(age_tab$pm_tf[train.test.ind$train]), color = "sd response"), linetype = "dashed", size = 1) +
    # Add annotations for each horizontal line
    annotate("text", x = 0, y = r.train + 0.001, label = paste("Ridge:", round(r.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = l.train + 0.001, label = paste("Lasso:", round(l.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = gpr.train + 0.001, label = paste("GPR:", round(gpr.train, 4)), hjust = 0, vjust = -0.5, color = "black") +
    annotate("text", x = 0, y = sd(age_tab$pm_tf[train.test.ind$train]) + 0.001, label = paste("sd response:", round(sd(age_tab$pm_tf[train.test.ind$train]), 4)), hjust = 0, vjust = -0.5, color = "black") +
    scale_x_continuous(breaks = seq(0, num.it, by = 500)) +
    # ylim(0.88, 1.05) +
    theme_minimal() +
    labs(title = paste("Train RMSE (0th Percentile) for ", Class," Classes"), 
         x = "Iterations", y = "0th Percentile RMSE", color = "Reference Lines")
  
  print(train_plot)  # Print the Train RMSE plot
}
```
//[failed ext pred] oct30_pm_sm_gpnn_12init_bbs 20092556 ==> doing K = 2 gpnn, and also do min recording AND ext predictions too.
//`pm_sm_gpnn_12init_bbs_ext.R` created to make a ext predition 20276109==> [without run 10]

//oct28_pm_sm_gpols_12init_bbs K8 20235715  ===> K = 8 gpols, also do ext predictions too... only need to look at run 6, only record 350 epochs.
//aug9_pm_sm_gpols_12init_bbs K3 20309839 ==> note the added K = 3 here, need to look at run 5


To do

run ext for gpgp, linear models

//re_pm_aug9_ridge_outpred_ext_noscale_, re_pm_aug9_lasso_outpred_ext_noscale_ 20338840
//re_aug9_pm_gpr_outpred_ext  20338979

//aug9_pm_sm_gpgp_12init_bbs ==> K =3 with ext 20344749
also needa do gpgp with k = 2
//oct31_pm_sm_gpgp_12init ==> K = 2 20345492
oct31_pm_sm_gpgp_12init_bbs 20430017 ==> K = 2 with ext 

#1 Nov

##ext predictions
```{r}
library(feather)
age_tab <-  as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <-  read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
train.test.ind$unseen <- setdiff(seq(nrow(age_tab)),c(train.test.ind$train,train.test.ind$test))
```

###linear models
Ridge
```{r}
out.pred <-  as.matrix(read.csv('/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_outpred_ext_noscale_1.csv'))
print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[train.test.ind$unseen]))^2)),3)))
```
LASSO
```{r}
out.pred <-  as.matrix(read.csv('/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_outpred_ext_noscale_1.csv'))
print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[train.test.ind$unseen]))^2)),3)))
```
GPR
```{r}
out.pred <-  as.matrix(read.csv('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_outpred_ext_noscale_1.csv'))
print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[train.test.ind$unseen]))^2)),3)))
```

###GP-OLS K = 3
some saving is wrong, out.pred is hugge
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_K3_min_outpred_ext_test__jobid_5.feather'))
out.pred <- out.pred[, (ncol(out.pred) - length(age_tab$pm_tf) +1 ):ncol(out.pred)]
print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```

###GP-OLS K = 8

```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K8_min_outpred_ext_test__jobid_6.feather'))
out.pred <- out.pred[, (ncol(out.pred) - length(age_tab$pm_tf) +1 ):ncol(out.pred)]

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```

###GPNN K = 2
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)
# Print the results dataframe
print(results_df)
```
Run 8 is the best
`oct30_pm_sm_gpnn_12init_bbs_min_outpred_ext_test` 20464897
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_bbs_min_outpred_ext_test__jobid_8.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_bbs_min_outpred_ext__jobid_10.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```

###GPGP K = 3
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)
# Print the results dataframe
print(results_df)
```
run 4
aug9_pm_sm_gpgp_12init_bbs ext 20466692 ==> just for predicting ext
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_min_outpred_ext_test__jobid_4.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```

###GPGP K = 2
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct31_pm_sm_gpgp_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct31_pm_sm_gpgp_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)
# Print the results dataframe
print(results_df)
```
//oct31_pm_sm_gpgp_12init_bbs ext  20466674
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct31_pm_sm_gpgp_12init_bbs_min_outpred_ext_test__jobid_2.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```


##Compiling
```{r}
# Initialize a data frame to store the results
results <- data.frame(
  Method = character(),
  Training_RMSE = numeric(),
  Test_RMSE = numeric(),
  Unseen_RMSE = numeric(),
  Training_R2 = numeric(),
  Test_R2 = numeric(),
  Unseen_R2 = numeric(),
  stringsAsFactors = FALSE
)

# Define helper functions to calculate RMSE and R^2
calculate_rmse <- function(pred, true_values, indices) {
  sqrt(mean((true_values[indices] - pred[indices])^2))
}

calculate_r2 <- function(pred, true_values, indices) {
  rss <- sum((true_values[indices] - pred[indices])^2)
  tss <- sum((true_values[indices] - mean(true_values[indices]))^2)
  (1 - rss / tss) * 100
}

# Load common data and indices
library(feather)
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))
train.test.ind <- list()
train.test.ind$test <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_test_index.csv')$x
train.test.ind$train <- read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x
train.test.ind$unseen <- setdiff(seq(nrow(age_tab)), c(train.test.ind$train, train.test.ind$test))

# Define methods and file paths
methods <- list(
  "Ridge" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_ridge_outpred_ext_noscale_1.csv',
  "LASSO" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_pm_aug9_lasso_outpred_ext_noscale_1.csv',
  "GPR" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpr_outpred_ext_noscale_1.csv',
  "GP-OLS K=2" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K2_min_outpred_ext_test__jobid_1.feather',
  "GP-OLS K=2 (Convergence)" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_outpred_ext_test__jobid_1.feather',
  "GP-OLS K=3" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_K3_min_outpred_ext_test__jobid_5.feather',
  "GP-OLS K=8" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct28_pm_sm_gpols_12init_bbs_K8_min_outpred_ext_test__jobid_6.feather',
  "GPNN K=2" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_bbs_min_outpred_ext_test__jobid_8.feather',
  "GPNN K=3" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_min_outpred_ext__jobid_10.feather',
  "GPGP K=2" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct31_pm_sm_gpgp_12init_bbs_min_outpred_ext_test__jobid_2.feather',
  "GPGP K=3" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpgp_12init_bbs_min_outpred_ext_test__jobid_4.feather',
  "GP-OLS img-only" = '/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_min_outpred_ext_test__jobid_9.feather'
)

# Calculate RMSE and R^2 for each method
for (method in names(methods)) {
  # Load the prediction data
  file_path <- methods[[method]]
  
  # Check if method is Ridge, LASSO, or GPR (vector predictions)
  if (method %in% c("Ridge", "LASSO", "GPR")) {
    out.pred <- read.csv(file_path)$x  # Read as vector directly
    pred_values <- out.pred
  } else {
    # Load feather or CSV as a matrix
    out.pred <- if (grepl("feather$", file_path)) {
      as.matrix(read_feather(file_path))
    } else {
      as.matrix(read.csv(file_path))
    }
    
    # Subset to the final columns matching the length of age_tab$pm_tf if necessary
    if (ncol(out.pred) > length(age_tab$pm_tf)) {
      out.pred <- out.pred[, (ncol(out.pred) - length(age_tab$pm_tf) + 1):ncol(out.pred)]
    }
    pred_values <- out.pred[2, ]  # Use second row for predictions
  }
  
  # Calculate RMSE for training, test, and unseen indices
  train_rmse <- calculate_rmse(pred_values, age_tab$pm_tf, train.test.ind$train)
  test_rmse <- calculate_rmse(pred_values, age_tab$pm_tf, train.test.ind$test)
  unseen_rmse <- calculate_rmse(pred_values, age_tab$pm_tf, train.test.ind$unseen)
  
  # Calculate R^2 for training, test, and unseen indices
  train_r2 <- calculate_r2(pred_values, age_tab$pm_tf, train.test.ind$train)
  test_r2 <- calculate_r2(pred_values, age_tab$pm_tf, train.test.ind$test)
  unseen_r2 <- calculate_r2(pred_values, age_tab$pm_tf, train.test.ind$unseen)
  
  # Append results to the data frame
  results <- rbind(results, data.frame(
    Method = method,
    Training_RMSE = round(train_rmse, 3),
    Test_RMSE = round(test_rmse, 3),
    Unseen_RMSE = round(unseen_rmse, 3),
    Training_R2 = round(train_r2, 3),
    Test_R2 = round(test_r2, 3),
    Unseen_R2 = round(unseen_r2, 3)
  ))
}

# Print the compiled results table
print(results)

```
It's missing GPNN K = 3
//aug9_pm_sm_gpnn_12init_bbs _ext_  20495259
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(2,3,8,9,10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)
# Print the results dataframe
print(results_df)
```

Run 3
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpnn_12init_bbs_min_outpred_ext__jobid_10.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```
they don't add up again. The numbers are different as I record minimum after 8 iterations, but min achieved at iteration 3

Exporting
```{r}
# Define the file path for the CSV
file_path <- "/well/nichols/users/qcv214/KGPNN/cog/tables/results_pm_1nov.csv"
# Export the table as a CSV file
write.csv(results, file = file_path, row.names = FALSE)
# Confirm the file path
cat("Results exported to:", file_path)
```

Imaging only
//[ext wasn't recorded properly] aug9_pm_gpols_12init_bbs 20495343 

//aug9_pm_gpols_12init_bbs _ext_ 20501874
```{r}
# Assuming res.mat is already populated from previous code
num.it <- 500 * 4
runs <- c(1:10)
res.mat <- array(, dim = c(4, length(runs), num.it))

# Load the data into the res.mat array
for(i in runs){
    res.mat[1:2, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_loss__jobid_", i, ".csv")))[,1:num.it]
    res.mat[3:4, which(i == runs), ] <- as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_rsq__jobid_", i, ".csv")))[,1:num.it]
}

# Find the location of the minimum test MSE (second variable, second dimension)
test_mse <- res.mat[2, , ]  # Extract test MSE (second variable)
min_mse_idx <- which(test_mse == min(test_mse), arr.ind = TRUE)  # Find location of minimum MSE

# Extract the corresponding run and iteration
run_idx <- min_mse_idx[1]  # Row index corresponding to the run
iter_idx <- min_mse_idx[2]  # Column index corresponding to the iteration

# Retrieve the values for training RMSE, test RMSE, training R^2, and test R^2 at the minimum MSE location
training_rmse <- sqrt(res.mat[1, run_idx, iter_idx])  # Training RMSE (sqrt of training MSE)
test_rmse <- sqrt(res.mat[2, run_idx, iter_idx])  # Test RMSE (sqrt of test MSE)
training_rsq <- res.mat[3, run_idx, iter_idx]  # Training R^2
test_rsq <- res.mat[4, run_idx, iter_idx]  # Test R^2

# Create a dataframe to display the results
results_df <- data.frame(
  Run = run_idx,
  Iteration = iter_idx,
  'Training RMSE' = round(training_rmse, 4),
  'Test RMSE' = round(test_rmse, 4),
  'Training R^2' = round(training_rsq, 4),
  'Test R^2' = round(test_rsq, 4)
)
# Print the results dataframe
print(results_df)
```
Run 9
```{r}
out.pred <-  as.matrix(read_feather('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_gpols_12init_bbs_min_outpred_ext_test__jobid_9.feather'))

print(paste0("training rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$train]-c(out.pred[2,train.test.ind$train]))^2)),3)))
print(paste0("test rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$test]-c(out.pred[2,train.test.ind$test]))^2)),3)))
print(paste0("unseen rmse: ", round( sqrt(mean((age_tab$pm_tf[train.test.ind$unseen]-c(out.pred[2,train.test.ind$unseen]))^2)),3)))
```


I also need the gender version


#2 Nov

##Compiling figures
###PM GP-OLS K = 2
Distributions
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1, 2, 10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Squared_Error', 'Class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(Class) %>%
    summarise(median_pred = median(Prediction, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(Class)
  
  # Reorder the `Class` factor based on the median `Prediction`
  age.in$Class <- factor(age.in$Class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(Class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$Class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$Class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(Age, Sex, Deprivation_Index, Prediction, Squared_Error, Pair_Matching), names_to = "variable", values_to = "value")
  
  # Calculate median values for annotation
  medians <- age.in_long %>%
    group_by(Class, variable) %>%
    summarise(median_value = round(median(value, na.rm = TRUE), 3), .groups = 'drop')
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = Class, y = value, fill = Class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Non-imaging Covariates, Labeled by Class for Run #", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
  # Add median text above each box in bold
  geom_text(data = medians, aes(label = median_value, y = median_value), 
            position = position_dodge(width = 0.75), vjust = -0.25, color = "black", size = 3, fontface = "bold")
  
  print(p)
}
```
let's do run 1,2,10 ==> arbitrarily.

Predictions
```{r}
library(dplyr)
library(ggplot2)
library(feather)
library(patchwork)  # For combining plots

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1,2,10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (age.in$pm_tf - dat.in$pred), as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Residual', 'Class')

  ### 1. Scatter Plot: pm_tf (Truth) vs pred (Fitted)
  truth_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Pair_Matching, color = Class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # Add a y=x line
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred")) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Fitted vs Truth", x = "Fitted Values", y = "True Values", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### 2. Scatter Plot: SqError vs pred (Fitted)
  sqerror_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Residual, color = Class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred")) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Residual vs Fitted", x = "Fitted Values", y = "Residual", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### Combine the two plots using patchwork
  combined_plot <- truth_vs_fitted_plot + sqerror_vs_fitted_plot + 
    plot_layout(ncol = 2) +  # Arrange the two plots side by side
    plot_annotation(title = paste("Scatter Plots for Run", r))  # Add a main title for the combined plot

  print(combined_plot)
}
```


```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(patchwork)

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1, 2, 10)

# Set consistent colors for the classes
class_colors <- c("1" = "lightblue4", "2" = "deeppink3")

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Squared_Error', 'Class')
  
  # Calculate class sizes and labels for legends
  class_sizes <- age.in %>%
    group_by(Class) %>%
    summarise(count = n())
  class_labels <- paste0(class_sizes$Class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$Class
  
  # Boxplot with medians
  age.in_long <- pivot_longer(age.in, cols = c(Age, Sex, Deprivation_Index, Prediction, Squared_Error, Pair_Matching), names_to = "variable", values_to = "value")
  medians <- age.in_long %>%
    group_by(Class, variable) %>%
    summarise(median_value = round(median(value, na.rm = TRUE), 3), .groups = 'drop')

  p <- ggplot(age.in_long, aes(x = Class, y = value, fill = Class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Non-imaging Covariates, Labeled by Class for Run #", r),
         x = "Class",
         y = "Value") +
    scale_fill_manual(values = class_colors, labels = class_labels) +
    geom_text(data = medians, aes(label = median_value, y = median_value), 
              position = position_dodge(width = 0.75), vjust = -0.25, color = "black", size = 3, fontface = "bold")
  print(p)

  # Scatter Plots with Sample Size Annotations
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (age.in$pm_tf - dat.in$pred), as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Residual', 'Class')

  # Scatter plot: pm_tf (Truth) vs pred (Fitted)
  truth_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Pair_Matching, color = Class)) +
    geom_point(alpha = 0.7) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
    scale_color_manual(values = class_colors, labels = class_labels) +
    theme_minimal() +
    labs(title = "Fitted vs Truth", x = "Fitted Values", y = "True Values", color = "Class") +
    theme(legend.position = "right")

  # Scatter plot: SqError vs pred (Fitted)
  sqerror_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Residual, color = Class)) +
    geom_point(alpha = 0.7) +
    scale_color_manual(values = class_colors, labels = class_labels) +
    theme_minimal() +
    labs(title = "Residual vs Fitted", x = "Fitted Values", y = "Residual", color = "Class") +
    theme(legend.position = "right")

  # Combine scatter plots
  combined_plot <- truth_vs_fitted_plot + sqerror_vs_fitted_plot +
    plot_layout(ncol = 2) +
    plot_annotation(title = paste("Scatter Plots for Run", r))
  
  print(combined_plot)
}
```
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)
library(patchwork)

# Directory for saving plots
output_dir <- "/well/nichols/users/qcv214/KGPNN/cog/figs"

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1, 2, 10)

# Set consistent colors for the classes
class_colors <- c("1" = "lightblue4", "2" = "deeppink3")

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug22_pm_sm_gpols_12init_bbs_K2_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Squared_Error', 'Class')
  
  # Calculate class sizes and labels for legends
  class_sizes <- age.in %>%
    group_by(Class) %>%
    summarise(count = n())
  class_labels <- paste0(class_sizes$Class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$Class
  
  # Boxplot with medians
  age.in_long <- pivot_longer(age.in, cols = c(Age, Sex, Deprivation_Index, Prediction, Squared_Error, Pair_Matching), names_to = "variable", values_to = "value")
  medians <- age.in_long %>%
    group_by(Class, variable) %>%
    summarise(median_value = round(median(value, na.rm = TRUE), 3), .groups = 'drop')

  p <- ggplot(age.in_long, aes(x = Class, y = value, fill = Class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Non-imaging Covariates, Labeled by Class for Run #", r),
         x = "Class",
         y = "Value") +
    scale_fill_manual(values = class_colors, labels = class_labels) +
  geom_text(data = medians, aes(label = median_value, y = median_value), 
            position = position_dodge(width = 0.75), vjust = -0.25, color = "black", size = 3, fontface = "bold")
  
  # Save boxplot as PDF
  ggsave(filename = file.path(output_dir, paste0("boxplot_run_", r, ".pdf")), plot = p, width = 10, height = 8)

  # Scatter Plots with Sample Size Annotations
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (age.in$pm_tf - dat.in$pred), as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Residual', 'Class')

  # Scatter plot: pm_tf (Truth) vs pred (Fitted)
  truth_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Pair_Matching, color = Class)) +
    geom_point(alpha = 0.7) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
    scale_color_manual(values = class_colors, labels = class_labels) +
    theme_minimal() +
    labs(title = "Fitted vs Truth", x = "Fitted Values", y = "True Values", color = "Class") +
    theme(legend.position = "right")

  # Scatter plot: SqError vs pred (Fitted)
  sqerror_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Residual, color = Class)) +
    geom_point(alpha = 0.7) +
    scale_color_manual(values = class_colors, labels = class_labels) +
    theme_minimal() +
    labs(title = "Residual vs Fitted", x = "Fitted Values", y = "Residual", color = "Class") +
    theme(legend.position = "right")

  # Combine scatter plots
  combined_plot <- truth_vs_fitted_plot + sqerror_vs_fitted_plot +
    plot_layout(ncol = 2) +
    plot_annotation(title = paste("Scatter Plots for Run", r))
  
  # Save scatter plot as PDF
  ggsave(filename = file.path(output_dir, paste0("scatter_plot_run_", r, ".pdf")), plot = combined_plot, width = 12, height = 6)
}
```

I still need to delete fitted vs truth?


###PM GP-OLS K = 3
Distributions
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(feather)  # Ensure this package is installed for reading feather files

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(1:10)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')
  
  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (dat.in$pred - age.in$pm_tf)^2, as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Squared_Error', 'Class')
  
  # Calculate the median `pred` for each `class`
  class_order <- age.in %>%
    group_by(Class) %>%
    summarise(median_pred = median(Prediction, na.rm = TRUE)) %>%
    arrange(median_pred) %>%
    pull(Class)
  
  # Reorder the `Class` factor based on the median `Prediction`
  age.in$Class <- factor(age.in$Class, levels = class_order)
  
  # Calculate class sizes
  class_sizes <- age.in %>%
    group_by(Class) %>%
    summarise(count = n())
  
  # Create a named vector for custom labels
  class_labels <- paste0(class_sizes$Class, " (n=", class_sizes$count, ")")
  names(class_labels) <- class_sizes$Class
  
  # Reshape the data for plotting
  age.in_long <- pivot_longer(age.in, cols = c(Age, Sex, Deprivation_Index, Prediction, Squared_Error, Pair_Matching), names_to = "variable", values_to = "value")
  
  # Calculate median values for annotation
  medians <- age.in_long %>%
    group_by(Class, variable) %>%
    summarise(median_value = round(median(value, na.rm = TRUE), 3), .groups = 'drop')
  
  # Create the boxplot with class sizes in the legend
  p <- ggplot(age.in_long, aes(x = Class, y = value, fill = Class)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free") +
    theme_minimal() +
    labs(title = paste("Boxplot of Non-imaging Covariates, Labeled by Class for Run #", r),
         x = "Class",
         y = "Value") +
    scale_fill_discrete(labels = class_labels) +
  # Add median text above each box in bold
  geom_text(data = medians, aes(label = median_value, y = median_value), 
            position = position_dodge(width = 0.75), vjust = -0.25, color = "black", size = 3, fontface = "bold")
  
  print(p)
}
```
5 has good interpretations Run 4,6,7 isn't great. Let's do 4,5,6

Predictions
```{r}
library(dplyr)
library(ggplot2)
library(feather)
library(patchwork)  # For combining plots

# Load the age and sex stratified data
age_tab <- as.data.frame(read_feather('/well/nichols/users/qcv214/KGPNN/cog/agesex_strat2.feather'))

# Load the number of test cases
n.test <- length(read.csv('/well/nichols/users/qcv214/KGPNN/cog/cog_train_index.csv')$x)
success.run <- c(4:6)

for (r in success.run) {
  # Load and process data
  dat.in <- as.data.frame(t(read_feather(paste0('/well/nichols/users/qcv214/KGPNN/cog/pile/re_aug9_pm_sm_gpols_12init_bbs_inpred__jobid_', r, '.feather'))))[, c(1:3)]
  dat.in <- tail(dat.in, n.test)
  colnames(dat.in) <- c('id_ind', 'pred', 'class')

  # Merge with age and sex information
  age.in <- age_tab[dat.in$id_ind, c('age', 'sex', 'DepInd', 'pm_tf')]
  age.in <- cbind(age.in, dat.in$pred, (age.in$pm_tf - dat.in$pred), as.factor(dat.in$class))
  colnames(age.in) <- c('Age', 'Sex', 'Deprivation_Index', 'Pair_Matching', 'Prediction', 'Residual', 'Class')

  ### 1. Scatter Plot: pm_tf (Truth) vs pred (Fitted)
  truth_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Pair_Matching, color = Class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # Add a y=x line
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred", "3" = 'orange')) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Fitted vs Truth", x = "Fitted Values", y = "True Values", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### 2. Scatter Plot: SqError vs pred (Fitted)
  sqerror_vs_fitted_plot <- ggplot(age.in, aes(x = Prediction, y = Residual, color = Class)) +
    geom_point(alpha = 0.7) +  # Use points with slight transparency
    scale_color_manual(values = c("1" = "darkblue", "2" = "darkred", "3" = 'orange')) +  # Dark blue for Class 1, Dark red for Class 2
    theme_minimal() +
    labs(title = "Residual vs Fitted", x = "Fitted Values", y = "Residual", color = "Class") +
    theme(legend.position = "right")  # Show legend on the right

  ### Combine the two plots using patchwork
  combined_plot <- truth_vs_fitted_plot + sqerror_vs_fitted_plot + 
    plot_layout(ncol = 2) +  # Arrange the two plots side by side
    plot_annotation(title = paste("Scatter Plots for Run", r))  # Add a main title for the combined plot

  print(combined_plot)
}
```

##SGLD results
Need unseen. Also need a comparison e.g. MALA and GPR? or if want fast then GPR is better 
SGLD data jitter ~ oct9 `pm_sm_gpols_12init_K_sgld_run.R` ==> need to rerun this for longer
specify how run is picked (min loss) the jitter needed. 
Maybe do need to do convergence test but need to explain that params from different runs are in different spaces.
Give the following comparison:
- GPR - no subclasses
- 2-GPOLS
- 4-GPOLS (maybe 3 as well) ==> need to check if MAP is correct
    -> i think easier to compare GPOLS k = 2 with gpgp and gpnn k = 2, and gpols-imaging only ----- we already identified _map_ for the bbs.
    -> but k = 4 can show unstability.

- No need for MALA since it's infeasible as shown in prev chapter.

###look at gpnn k = 2
```{r}
library(ggplot2)
library(tidyr)

num.it <- 500*4  # Total number of iterations
runs <- c(1:10)
res.mat <- matrix(, nrow = length(runs), ncol = num.it)

for (i in runs) {
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct30_pm_sm_gpnn_12init_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE for iterations 10 to 400
resdat <- as.data.frame(res.mat)[, 10:num.it]  # Use iterations 10 to 400
colnames(resdat) <- 10:num.it  # Update column names to reflect the actual iteration numbers
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(10, num.it, by = 500), limits = c(10, num.it)) +  # Set x-axis from 10 to 400 with breaks every 50
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
Same as gpgp, let's pick run with lowest at the end
```{r}
which.min(res.mat[,2000])
```

`pm_sm_gpnn_12init_sgld_run.R` oct30_pm_sm_gpnn_12init_sgld_lr99_run4  20637053

###look at gpgp k = 2
```{r}
library(ggplot2)
library(tidyr)

num.it <- 500*4  # Total number of iterations
runs <- c(1:10)
res.mat <- matrix(, nrow = length(runs), ncol = num.it)

for (i in runs) {
  res.mat[which(i == runs), ] <- t(as.matrix(read.csv(paste0("/well/nichols/users/qcv214/KGPNN/cog/pile/re_oct31_pm_sm_gpgp_12init_map__jobid_", i, ".csv"))))[, 1:num.it]
}

# Plotting Test RMSE for iterations 10 to 400
resdat <- as.data.frame(res.mat)  # Use iterations 10 to 400
colnames(resdat) <- 1:num.it  # Update column names to reflect the actual iteration numbers
resdat$row_id <- seq_len(nrow(resdat))

# Reshape the data to long format for ggplot
resdat_long <- pivot_longer(resdat, cols = -row_id, names_to = "column", values_to = "value")

# Create the line plot for Test RMSE
ggplot(resdat_long, aes(x = as.numeric(column), y = value, group = row_id, color = as.factor(row_id))) +
  geom_line() +  # Add geom_line() to draw the lines
  scale_x_continuous(breaks = seq(0, num.it, by = 500), limits = c(0, num.it)) +  # Set x-axis from 10 to 400 with breaks every 50
  theme_minimal() +
  labs(title = "Training neg logPosterior loss", x = "Iterations", y = "Value", color = "Run number")
```
We can see that RMSE shoot up from 0 it, but then it decreases very nicely over time. in this case let's pick from from
```{r}
which.min(resdat[,2000])
```
we'll do sgld on run 9.
//`pm_sm_gpgp_12init_sgld_run.R` oct31_pm_sm_gpgp_12init_sgld_lr99_run9 20636965

###for gpols K = 2
// nov4_pm_sm_gpols_12init_sgld_lr99_run2 20637068

###for gpols K = 3 
So the MAP isn't right but can still use the lowest map as the incorrect terms are all the same regardless of theta.
Maybe don't need??

###for gpols-img-only
`aug9_pm_gpols_12init_sgld` => already have results 


##Saliency, check for saliency
`sep24_pm/` `sep24_similar`
The code for plotting is commented under `pm_sm_gpols_12init_K_bbs.R` which works for K = 2. 
Let's just go with what we already have for saliency.
Contrast it against LASSO, Ridge, GPR ==> need to generate.
// `pm_ridgelasso.R` 20635580 added saliency for ridge and lasso, saving to /viz, aug9, created viz/ridgelasso to save the merged ridge and lasso over runs. 
gpr was always saved
it's not displaying the correct results. gpr and ridge are empty, but these have many non-zero coef tho
So, for ridge, it seems fine when I don't compile it and look at individual photos, but for GPR something is wrong. All saved images are nan.
Maybe I have to do `abs(c(beta_fit$NM))`
//Running  `pm_gpr` again with this 20646174 => still not working.. okay, apparently got an error that it's not same length

##Bimodal
oct29_pm_bi_gpols_init 20641619, 20653632 ==> observe the dimensions of each component in training hidden layer, since I think z.nb is the problem.
    so i thought it was the learning rate that was problem but it got update every epoch but my method failed within epoh. so 20691776 will attemp reducing starting lr
    20716821 => reduce lr and assess the mean magnitude of gradients .... so i think either i have exploding or diminishing gradient. Maybe I should just say if gradient < 1e-10 then don't update. I think fixing grad.loss should fix all this maybe
    20723998 ==> change grad.loss to 0 if < 1e-8



  